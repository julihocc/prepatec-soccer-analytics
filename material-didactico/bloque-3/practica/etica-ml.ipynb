{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee183a14",
   "metadata": {},
   "source": [
    "# Ética en Machine Learning: Detección y Mitigación de Sesgos\n",
    "\n",
    "## Objetivos\n",
    "- Comprender los conceptos de sesgo (bias) y justicia (fairness) en ML.\n",
    "- Identificar fuentes comunes de sesgo en datos y modelos.\n",
    "- Utilizar herramientas para medir el sesgo en modelos de clasificación.\n",
    "- Aplicar técnicas de pre-procesamiento y post-procesamiento para mitigar sesgos.\n",
    "- Evaluar el trade-off entre la precisión del modelo y la justicia.\n",
    "\n",
    "## Contexto\n",
    "En esta práctica, exploraremos cómo los modelos de machine learning pueden perpetuar o incluso amplificar sesgos existentes en los datos. Trabajaremos con un dataset sintético de decisiones de scouting para evaluar si un modelo de recomendación de fichajes es justo con respecto a la nacionalidad de los jugadores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f8c98",
   "metadata": {},
   "source": [
    "## 1. Preparación del Entorno\n",
    "\n",
    "### Instalación de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b52c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn plotly fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6761247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import plotly.graph_objects as go\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame,\n",
    "    selection_rate,\n",
    "    demographic_parity_difference,\n",
    "    equalized_odds_difference\n",
    ")\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c71c9f",
   "metadata": {},
   "source": [
    "## 2. Creación del Dataset con Sesgo Introducido\n",
    "\n",
    "Generaremos un dataset sintético donde la nacionalidad de un jugador influye en la decisión de ficharlo, introduciendo un sesgo deliberado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_players = 2000\n",
    "\n",
    "data = {\n",
    "    'age': np.random.randint(18, 30, n_players),\n",
    "    'potential': np.random.uniform(60, 95, n_players),\n",
    "    'performance_rating': np.random.uniform(6, 10, n_players),\n",
    "    'physical_score': np.random.uniform(50, 90, n_players),\n",
    "    'nationality': np.random.choice(['Local', 'Foreign'], n_players, p=[0.7, 0.3])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introducir sesgo: los jugadores locales tienen más probabilidad de ser fichados\n",
    "score = (df['potential'] + df['performance_rating']*10 + df['physical_score']) / 3\n",
    "probability = 1 / (1 + np.exp(-(score - 75)))\n",
    "\n",
    "# Aumentar la probabilidad para jugadores locales\n",
    "probability[df['nationality'] == 'Local'] *= 1.2\n",
    "probability = np.clip(probability, 0, 1)\n",
    "\n",
    "df['sign_decision'] = (probability > np.random.rand(n_players)).astype(int)\n",
    "\n",
    "# Guardar dataset\n",
    "df.to_csv('biased_scouting_decisions.csv', index=False)\n",
    "\n",
    "print(\"Dataset con sesgo creado:\")\n",
    "print(df.head())\n",
    "print(\"\\nDistribución de decisiones por nacionalidad:\")\n",
    "print(df.groupby('nationality')['sign_decision'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290a82a",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento del Modelo y Evaluación Inicial\n",
    "\n",
    "Entrenaremos un modelo de clasificación sin tener en cuenta el sesgo y evaluaremos su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "X = df.drop(['sign_decision', 'nationality'], axis=1)\n",
    "y = df['sign_decision']\n",
    "sensitive_features = df['nationality']\n",
    "\n",
    "X_train, X_test, y_train, y_test, sf_train, sf_test = train_test_split(\n",
    "    X, y, sensitive_features, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar modelo\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluación de precisión\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Precisión del modelo: {accuracy:.3f}\\n\")\n",
    "print(\"Reporte de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a412bba4",
   "metadata": {},
   "source": [
    "## 4. Medición del Sesgo con Fairlearn\n",
    "\n",
    "Utilizaremos `fairlearn` para cuantificar el sesgo en nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5502f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas de justicia\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score,\n",
    "    'selection_rate': selection_rate\n",
    "}\n",
    "\n",
    "metric_frame = MetricFrame(\n",
    "    metrics=metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=sf_test\n",
    ")\n",
    "\n",
    "print(\"Métricas por grupo:\")\n",
    "print(metric_frame.by_group)\n",
    "\n",
    "# Diferencia en paridad demográfica\n",
    "dpd = demographic_parity_difference(y_test, y_pred, sensitive_features=sf_test)\n",
    "print(f\"\\nDemographic Parity Difference: {dpd:.3f}\")\n",
    "\n",
    "# Diferencia en probabilidades igualadas\n",
    "eod = equalized_odds_difference(y_test, y_pred, sensitive_features=sf_test)\n",
    "print(f\"Equalized Odds Difference: {eod:.3f}\")\n",
    "\n",
    "# Visualización\n",
    "metric_frame.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[1, 2],\n",
    "    legend=False,\n",
    "    figsize=[8, 4],\n",
    "    ylim=[0,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b4834",
   "metadata": {},
   "source": [
    "## 5. Mitigación del Sesgo con Post-procesamiento\n",
    "\n",
    "Aplicaremos `ThresholdOptimizer` de `fairlearn` para ajustar los umbrales de decisión del modelo y mejorar la justicia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a569588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar el optimizador de umbral\n",
    "postprocess_est = ThresholdOptimizer(\n",
    "    estimator=model,\n",
    "    constraints=\"demographic_parity\",\n",
    "    objective=\"accuracy_score\",\n",
    "    prefit=True\n",
    ")\n",
    "\n",
    "# Entrenar el optimizador\n",
    "postprocess_est.fit(X_train, y_train, sensitive_features=sf_train)\n",
    "\n",
    "# Obtener predicciones ajustadas\n",
    "y_pred_post = postprocess_est.predict(X_test, sensitive_features=sf_test)\n",
    "\n",
    "# Evaluar el modelo mitigado\n",
    "print(\"--- Resultados después de la mitigación ---\")\n",
    "accuracy_post = accuracy_score(y_test, y_pred_post)\n",
    "print(f\"Precisión del modelo mitigado: {accuracy_post:.3f}\\n\")\n",
    "\n",
    "metric_frame_post = MetricFrame(\n",
    "    metrics=metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_post,\n",
    "    sensitive_features=sf_test\n",
    ")\n",
    "\n",
    "print(\"Métricas por grupo (mitigado):\")\n",
    "print(metric_frame_post.by_group)\n",
    "\n",
    "dpd_post = demographic_parity_difference(y_test, y_pred_post, sensitive_features=sf_test)\n",
    "print(f\"\\nDemographic Parity Difference (mitigado): {dpd_post:.3f}\")\n",
    "\n",
    "eod_post = equalized_odds_difference(y_test, y_pred_post, sensitive_features=sf_test)\n",
    "print(f\"Equalized Odds Difference (mitigado): {eod_post:.3f}\")\n",
    "\n",
    "# Visualización comparativa\n",
    "metric_frame_post.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[1, 2],\n",
    "    legend=False,\n",
    "    figsize=[8, 4],\n",
    "    ylim=[0,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d14f07",
   "metadata": {},
   "source": [
    "## 6. Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Explorar Diferentes Métricas de Justicia\n",
    "1. Investiga y aplica otras métricas de `fairlearn`, como `true_positive_rate_difference`.\n",
    "2. Cambia el `constraint` en `ThresholdOptimizer` a `equalized_odds` y analiza los resultados. ¿Cómo cambian la precisión y la justicia?\n",
    "\n",
    "### Ejercicio 2: Técnicas de Pre-procesamiento\n",
    "1. Investiga algoritmos de pre-procesamiento para mitigar sesgos, como `Reweighing` de `fairlearn`.\n",
    "2. Aplica `Reweighing` a los datos de entrenamiento y vuelve a entrenar el modelo. Compara los resultados con el post-procesamiento.\n",
    "\n",
    "### Ejercicio 3: Análisis de Trade-off\n",
    "1. Crea un gráfico que muestre la precisión del modelo vs. la métrica de justicia (e.g., `demographic_parity_difference`) para el modelo original y el mitigado.\n",
    "2. Discute el \"costo\" de la justicia en términos de pérdida de precisión. ¿Es aceptable en este contexto?\n",
    "\n",
    "## Reflexiones y Tareas\n",
    "\n",
    "1. **¿Qué otras fuentes de sesgo podrían existir en datos de scouting de fútbol?** (Ej: sesgo de selección, sesgo histórico).\n",
    "2. **¿Quién debería ser responsable de asegurar la justicia en los modelos de ML en un club de fútbol?**\n",
    "3. **¿Cómo comunicarías los resultados de un análisis de sesgo a una audiencia no técnica?**\n",
    "4. **Imagina que el modelo se usa para decidir a qué juveniles se les ofrece un contrato profesional. ¿Qué implicaciones éticas tiene esto?**\n",
    "\n",
    "### Recursos Adicionales\n",
    "\n",
    "- [Fairlearn Documentation](https://fairlearn.org/)\n",
    "- [Google's AI Fairness Resources](https://ai.google/responsibilities/responsible-ai-practices/)\n",
    "- [A Tutorial on Fairness in Machine Learning](https://arxiv.org/abs/1810.03882)\n",
    "\n",
    "---\n",
    "\n",
    "**¡Felicidades!** Has dado un paso importante para convertirte en un científico de datos responsable, aprendiendo a identificar, medir y mitigar sesgos en modelos de machine learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
