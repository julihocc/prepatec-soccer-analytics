---
jupyter:
  jupytext:
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.17.2
  kernelspec:
    display_name: .venv
    language: python
    name: python3
---

# Semana 13: ¬øC√≥mo sabemos si nuestras predicciones son realmente buenas?

## SESI√ìN 1: ¬øQu√© significa que un an√°lisis sea "confiable"? (50 min)

**Pregunta gu√≠a**: ¬øC√≥mo evaluamos la calidad de las decisiones de nuestros "entrenadores digitales"?

### ¬øHas notado c√≥mo eval√∫an a los entrenadores de f√∫tbol?

Imag√≠nate que eres periodista deportivo y debes evaluar a dos entrenadores del Barcelona despu√©s de 10 partidos:

**Entrenador A**:

- Gan√≥ 8 partidos, perdi√≥ 2
- Sus predicciones de alineaci√≥n fueron correctas en 8/10 casos

**Entrenador B**:

- Gan√≥ 7 partidos, perdi√≥ 3  
- Sus predicciones de alineaci√≥n fueron correctas en 7/10 casos

**Pregunta reflexiva**: ¬øEs suficiente solo mirar cu√°ntos aciertos tuvo cada uno? ¬øQu√© otros aspectos importan?

### M√°s all√° de solo contar aciertos

Cuando evaluamos predicciones deportivas, necesitamos considerar:

- ¬øQu√© tipo de errores cometi√≥?
- ¬øLos errores fueron en decisiones importantes o menores?
- ¬øQu√© tan seguro estaba de sus predicciones?

Hoy aprenderemos a medir estas cosas de manera precisa.

### Tipos de errores en las predicciones deportivas

Imag√≠nate que tu "entrenador digital" debe predecir si un jugador ser√° titular o suplente. Puede cometer dos tipos de errores:

**Error Tipo 1: "Falsa Alarma"**

- Predice: "Messi ser√° suplente"
- Realidad: "Messi fue titular"
- ¬øQu√© tan grave es este error?

**Error Tipo 2: "Se le pas√≥"**

- Predice: "Ansu Fati ser√° titular"  
- Realidad: "Ansu Fati fue suplente"
- ¬øEste error es igual de grave que el anterior?

**Pregunta reflexiva**: ¬øCrees que ambos errores tienen la misma importancia? ¬øEn qu√© situaciones ser√≠a peor un tipo de error que el otro?

### La matriz de confusi√≥n: nuestro tablero de an√°lisis

Para entender mejor los errores, los organizamos en una tabla especial que nos muestra:

- ¬øCu√°ntas veces acertamos completamente?
- ¬øCu√°ntas veces nos equivocamos y de qu√© manera?

Es como tener un reporte detallado del rendimiento de nuestro entrenador.

### Pr√°ctica inmediata: Evaluando las decisiones del Barcelona

Vamos a analizar las predicciones de nuestro "entrenador digital" sobre qui√©n deber√≠a ser titular en el Barcelona.

**Situaci√≥n**: Nuestro modelo analiz√≥ 15 jugadores y predijo qui√©nes ser√≠an titulares en el pr√≥ximo partido. Ahora comparamos sus predicciones con lo que realmente pas√≥.

**Datos que analizaremos:**

- Predicciones del modelo: ¬øQui√©n dijo que ser√≠a titular?
- Realidad del partido: ¬øQui√©n realmente fue titular?
- Tipos de aciertos y errores cometidos

**Pregunta gu√≠a**: ¬øC√≥mo podemos organizar esta informaci√≥n para entender mejor el rendimiento de nuestro modelo?

```python
# Importamos las herramientas necesarias
import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Datos del an√°lisis: predicciones vs realidad
jugadores_barcelona = [
    'Ter Stegen', 'Balde', 'Araujo', 'Kounde', 'Cancelo',
    'De Jong', 'Gavi', 'Pedri', 'Raphinha', 'Lewandowski', 'Ferran Torres',
    'Ansu Fati', 'Sergi Roberto', 'Christensen', 'Marcos Alonso'
]

# 1 = Titular, 0 = Suplente
predicciones_modelo = [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]  # Lo que predijo nuestro modelo
realidad_partido = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]     # Lo que realmente pas√≥

# Creamos una tabla para ver las comparaciones
analisis = pd.DataFrame({
    'Jugador': jugadores_barcelona,
    'Prediccion_Modelo': predicciones_modelo,
    'Realidad_Partido': realidad_partido,
    'Prediccion_Texto': ['Titular' if x == 1 else 'Suplente' for x in predicciones_modelo],
    'Realidad_Texto': ['Titular' if x == 1 else 'Suplente' for x in realidad_partido]
})

print("An√°lisis de predicciones del Barcelona:")
print(analisis)
print(f"\nTotal de jugadores analizados: {len(analisis)}")
```

### ¬øD√≥nde acert√≥ y d√≥nde se equivoc√≥ nuestro modelo?

Mirando la tabla anterior, vamos a clasificar cada caso:

**Pregunta reflexiva**: ¬øPuedes identificar qu√© jugadores fueron predichos correctamente y cu√°les incorrectamente?

### Clasificando los resultados

1. **Acierto Total**: Predijo titular y fue titular (o predijo suplente y fue suplente)
2. **Error Tipo 1**: Predijo suplente pero fue titular
3. **Error Tipo 2**: Predijo titular pero fue suplente

### Creando nuestra matriz de confusi√≥n

La matriz de confusi√≥n es como un marcador detallado que nos dice:

- ¬øCu√°ntos casos predijimos correctamente?
- ¬øQu√© tipos de errores cometimos?
- ¬øHay un patr√≥n en nuestros errores?

```python
# Creamos la matriz de confusi√≥n
matriz_confusion = confusion_matrix(realidad_partido, predicciones_modelo)

print("Matriz de Confusi√≥n:")
print("Filas = Realidad, Columnas = Predicci√≥n")
print(matriz_confusion)

# Vamos a entender qu√© significa cada n√∫mero
titulares_reales_predecidos_titulares = matriz_confusion[1, 1]  # Aciertos de titulares
suplentes_reales_predecidos_suplentes = matriz_confusion[0, 0]  # Aciertos de suplentes
titulares_reales_predecidos_suplentes = matriz_confusion[1, 0]  # Error: era titular, dijimos suplente
suplentes_reales_predecidos_titulares = matriz_confusion[0, 1]  # Error: era suplente, dijimos titular

print(f"\nüìä Desglose de resultados:")
print(f"‚úÖ Titulares correctos: {titulares_reales_predecidos_titulares}")
print(f"‚úÖ Suplentes correctos: {suplentes_reales_predecidos_suplentes}")
print(f"‚ùå Se nos escap√≥ un titular: {titulares_reales_predecidos_suplentes}")
print(f"‚ùå Predijimos titular pero fue suplente: {suplentes_reales_predecidos_titulares}")

# Calculamos la precisi√≥n total
precision_total = accuracy_score(realidad_partido, predicciones_modelo)
print(f"\nüéØ Precisi√≥n total del modelo: {precision_total*100:.1f}%")
```

## SESI√ìN 2: ¬øC√≥mo medimos aspectos espec√≠ficos del rendimiento? (50 min)

**Pregunta gu√≠a**: ¬øQu√© otras medidas nos ayudan a evaluar qu√© tan bueno es nuestro modelo?

### Analog√≠a deportiva: Evaluando a un delantero

Imag√≠nate que eval√∫as a dos delanteros del Barcelona despu√©s de 10 partidos:

**Delantero A**:

- Intent√≥ 20 disparos, marc√≥ 8 goles
- ¬øQu√© tan "efectivo" es? 8/20 = 40%

**Delantero B**:

- Intent√≥ 10 disparos, marc√≥ 6 goles  
- ¬øQu√© tan "efectivo" es? 6/10 = 60%

**Pregunta reflexiva**: ¬øQui√©n es mejor? ¬øEl que marca m√°s goles totales o el que tiene mejor efectividad?

### Aplicando esto a nuestros modelos

De la misma manera, podemos medir:

- **Efectividad con titulares**: De los que dijimos que ser√≠an titulares, ¬øcu√°ntos realmente lo fueron?
- **Capacidad de detecci√≥n**: De los titulares reales, ¬øa cu√°ntos logramos identificar?

Estas medidas nos dan una perspectiva m√°s completa del rendimiento.

```python
# Calculamos m√©tricas espec√≠ficas usando t√©rminos comprensibles

# 1. Efectividad con titulares (Precision en t√©rminos t√©cnicos)
# De los que predijimos como titulares, ¬øcu√°ntos realmente lo fueron?
predecidos_titulares = sum(predicciones_modelo)
titulares_correctos = titulares_reales_predecidos_titulares
efectividad_titulares = titulares_correctos / predecidos_titulares if predecidos_titulares > 0 else 0

# 2. Capacidad de detecci√≥n (Recall en t√©rminos t√©cnicos)  
# De los titulares reales, ¬øa cu√°ntos logramos identificar?
titulares_totales_reales = sum(realidad_partido)
capacidad_deteccion = titulares_correctos / titulares_totales_reales if titulares_totales_reales > 0 else 0

print("üìà M√©tricas espec√≠ficas de nuestro modelo:")
print(f"\n1. Efectividad con titulares:")
print(f"   - Predijimos {predecidos_titulares} jugadores como titulares")
print(f"   - {titulares_correctos} realmente fueron titulares")
print(f"   - Efectividad: {efectividad_titulares*100:.1f}%")

print(f"\n2. Capacidad de detecci√≥n:")
print(f"   - Hab√≠a {titulares_totales_reales} titulares reales en el partido")
print(f"   - Identificamos correctamente a {titulares_correctos}")
print(f"   - Capacidad de detecci√≥n: {capacidad_deteccion*100:.1f}%")

print(f"\nüéØ Precisi√≥n general: {precision_total*100:.1f}%")
```

### ¬øQu√© nos dicen estos n√∫meros?

**Pregunta reflexiva**: Mirando los resultados anteriores, ¬øqu√© fortalezas y debilidades tiene nuestro modelo?

### Interpretando nuestras m√©tricas

**Efectividad con titulares (Precision)**

- **Alta efectividad**: Cuando dice "titular", casi siempre acierta
- **Baja efectividad**: Predice muchos titulares que terminan siendo suplentes
- **En nuestro caso**: ¬øQu√© tan confiables son las predicciones de "titular"?

**Capacidad de detecci√≥n (Recall)**

- **Alta capacidad**: Identifica a la mayor√≠a de los titulares reales
- **Baja capacidad**: Se le "escapan" muchos titulares verdaderos
- **En nuestro caso**: ¬øCu√°ntos titulares reales logra detectar?

### ¬øCu√°l es m√°s importante?

Depende del contexto:

- **Para un DT**: Prefiere alta capacidad de detecci√≥n (no perderse ning√∫n buen jugador)
- **Para un analista**: Prefiere alta efectividad (sus recomendaciones deben ser precisas)

¬øQu√© preferir√≠as t√∫?

```python
# Creamos una visualizaci√≥n clara de nuestra matriz de confusi√≥n
plt.figure(figsize=(8, 6))

# Creamos el mapa de calor
sns.heatmap(matriz_confusion, 
            annot=True, 
            fmt='d', 
            cmap='Blues',
            xticklabels=['Predijo: Suplente', 'Predijo: Titular'],
            yticklabels=['Real: Suplente', 'Real: Titular'])

plt.title('Matriz de Confusi√≥n - Predicciones del Barcelona', size=14, pad=20)
plt.xlabel('Predicciones de nuestro modelo', size=12)
plt.ylabel('Realidad del partido', size=12)

# A√±adimos explicaciones en el gr√°fico
plt.text(0.5, -0.1, '‚úÖ Esquina inferior izquierda y superior derecha = ACIERTOS', 
         transform=plt.gca().transAxes, ha='center', size=10, color='green')
plt.text(0.5, -0.15, '‚ùå Esquina superior izquierda e inferior derecha = ERRORES', 
         transform=plt.gca().transAxes, ha='center', size=10, color='red')

plt.tight_layout()
plt.show()

print("üéØ ¬øQu√© observas en la matriz?")
print("- Los n√∫meros m√°s altos deber√≠an estar en la diagonal (aciertos)")
print("- Los n√∫meros en las esquinas opuestas son nuestros errores")
```

## SESI√ìN 3: ¬øC√≥mo usamos esta informaci√≥n para mejorar? (50 min)

**Pregunta gu√≠a**: ¬øQu√© podemos hacer con estos resultados para tomar mejores decisiones?

### Diagn√≥stico del rendimiento de nuestro modelo

Bas√°ndonos en nuestro an√°lisis, podemos diagnosticar:

**¬øNuestro modelo es demasiado "conservador"?**

- Si predice pocos titulares pero casi siempre acierta
- Alta efectividad, baja capacidad de detecci√≥n

**¬øNuestro modelo es demasiado "arriesgado"?**  

- Si predice muchos titulares pero se equivoca frecuentemente
- Baja efectividad, alta capacidad de detecci√≥n

**Pregunta reflexiva**: Bas√°ndote en nuestros resultados, ¬øc√≥mo describir√≠as el comportamiento de nuestro modelo?

### Comparando con diferentes estrategias

Vamos a comparar nuestro modelo con dos estrategias extremas:

1. **Estrategia conservadora**: Solo predecir titulares cuando est√©s muy seguro
2. **Estrategia arriesgada**: Predecir titular para la mayor√≠a de jugadores

¬øCu√°l crees que tendr√° mejor rendimiento general?

```python
# Vamos a comparar nuestro modelo con estrategias alternativas

# Estrategia 1: Conservadora (solo los m√°s obvios como titulares)
estrategia_conservadora = [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]  # Solo 10 titulares

# Estrategia 2: Arriesgada (la mayor√≠a como titulares)  
estrategia_arriesgada = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]   # 12 titulares

# Calculamos m√©tricas para cada estrategia
def calcular_metricas(predicciones, realidad, nombre):
    precision = accuracy_score(realidad, predicciones)
    matriz = confusion_matrix(realidad, predicciones)
    
    if len(matriz) == 2:  # Verificamos que tengamos ambas clases
        tp = matriz[1, 1]  # Titulares correctos
        predecidos_como_titulares = sum(predicciones)
        titulares_reales = sum(realidad)
        
        efectividad = tp / predecidos_como_titulares if predecidos_como_titulares > 0 else 0
        deteccion = tp / titulares_reales if titulares_reales > 0 else 0
    else:
        efectividad = deteccion = 0
    
    print(f"\nüîç {nombre}:")
    print(f"   Precisi√≥n general: {precision*100:.1f}%")
    print(f"   Efectividad con titulares: {efectividad*100:.1f}%")
    print(f"   Capacidad de detecci√≥n: {deteccion*100:.1f}%")
    
    return precision, efectividad, deteccion

# Comparamos todas las estrategias
print("üìä Comparaci√≥n de estrategias:")

metricas_original = calcular_metricas(predicciones_modelo, realidad_partido, "Nuestro Modelo Original")
metricas_conservadora = calcular_metricas(estrategia_conservadora, realidad_partido, "Estrategia Conservadora")
metricas_arriesgada = calcular_metricas(estrategia_arriesgada, realidad_partido, "Estrategia Arriesgada")
```

### S√≠ntesis de la Semana 13: ¬øQu√© hemos descubierto sobre evaluaci√≥n?

**Pregunta final de reflexi√≥n**: ¬øPor qu√© es importante medir m√°s que solo el porcentaje de aciertos?

### Lo que aprendimos hoy

1. **Los errores no son todos iguales**
   - Equivocarse con un titular es diferente a equivocarse con un suplente
   - El contexto determina qu√© tipo de error es m√°s costoso

2. **M√∫ltiples m√©tricas dan una visi√≥n completa**
   - Precisi√≥n general: rendimiento promedio
   - Efectividad: confiabilidad de predicciones positivas
   - Capacidad de detecci√≥n: habilidad para encontrar casos importantes

3. **El balance es clave**
   - No existe la estrategia perfecta
   - Debemos elegir seg√∫n nuestros objetivos espec√≠ficos

### Aplicaci√≥n pr√°ctica

**En el f√∫tbol real, esto nos ayuda a:**

- Evaluar sistemas de an√°lisis de jugadores
- Comparar diferentes m√©todos de predicci√≥n
- Tomar decisiones m√°s informadas sobre fichajes
- Mejorar constantemente nuestros modelos

### Conexi√≥n con el pr√≥ximo tema

**Pregunta puente**: Ahora que sabemos evaluar qu√© tan buenos son nuestros modelos, ¬øc√≥mo podr√≠amos mejorar la informaci√≥n que les damos para que sean a√∫n mejores?

La pr√≥xima semana exploraremos c√≥mo crear mejores caracter√≠sticas para nuestros modelos de predicci√≥n.
