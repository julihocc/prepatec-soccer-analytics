{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed11b6e",
   "metadata": {},
   "source": [
    "# Modelos Avanzados de Machine Learning para Análisis Futbolístico\n",
    "\n",
    "## Objetivos del Notebook\n",
    "\n",
    "Este notebook implementa técnicas avanzadas de machine learning para el análisis predictivo en fútbol, utilizando el dataset \"champs\" y expandiendo el análisis del Bloque 1 con modelos más sofisticados.\n",
    "\n",
    "### Contenido del Bloque 2:\n",
    "\n",
    "1. **Feature Engineering Avanzado**\n",
    "2. **Implementación de Múltiples Algoritmos**\n",
    "3. **Evaluación Comparativa de Modelos**\n",
    "4. **Optimización de Hiperparámetros**\n",
    "5. **Análisis de Importancia de Variables**\n",
    "6. **Validación Cruzada y Robustez**\n",
    "7. **Interpretación de Resultados**\n",
    "8. **Despliegue de Modelos**\n",
    "\n",
    "### Algoritmos a Implementar:\n",
    "- **Regresión Logística** (baseline)\n",
    "- **Random Forest** (ensemble method)\n",
    "- **Support Vector Machine** (SVM)\n",
    "- **Gradient Boosting** (XGBoost)\n",
    "- **Redes Neuronales** (MLPClassifier)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc36e5",
   "metadata": {},
   "source": [
    "## 1. Importar Librerías y Configuración Inicial\n",
    "\n",
    "Importamos las librerías necesarias para el análisis avanzado de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b97126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías básicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librerías de machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                           confusion_matrix, classification_report, roc_auc_score, roc_curve)\n",
    "\n",
    "# Algoritmos de clasificación\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Librerías adicionales\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✅ Todas las librerías importadas exitosamente\")\n",
    "print(f\"📊 Versión de scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"🐍 Versión de Python: {sys.version}\")\n",
    "print(f\"📅 Fecha de ejecución: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c248be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos desde el Bloque 1 o generar datos de ejemplo\n",
    "print(\"🔄 Cargando dataset...\")\n",
    "\n",
    "try:\n",
    "    # Intentar cargar el dataset real\n",
    "    df = pd.read_csv('../recursos/champs.csv')\n",
    "    print(\"✅ Dataset 'champs' cargado desde archivo\")\n",
    "except FileNotFoundError:\n",
    "    # Generar dataset de ejemplo más completo para demostración\n",
    "    print(\"📝 Generando dataset de ejemplo...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_matches = 1000\n",
    "    teams = ['Real Madrid', 'Barcelona', 'Atletico Madrid', 'Valencia', 'Sevilla', \n",
    "             'Athletic Bilbao', 'Real Sociedad', 'Villarreal', 'Betis', 'Getafe']\n",
    "    \n",
    "    # Generar datos base\n",
    "    data = {\n",
    "        'match_id': range(1, n_matches + 1),\n",
    "        'date': pd.date_range('2022-01-01', periods=n_matches, freq='D'),\n",
    "        'home_team': np.random.choice(teams, n_matches),\n",
    "        'away_team': np.random.choice(teams, n_matches),\n",
    "        'home_goals': np.random.poisson(1.3, n_matches),\n",
    "        'away_goals': np.random.poisson(1.1, n_matches),\n",
    "        'league': np.random.choice(['La Liga', 'Copa del Rey', 'Champions'], n_matches, p=[0.6, 0.2, 0.2]),\n",
    "        'season': np.random.choice(['2022-23', '2023-24'], n_matches),\n",
    "        'matchday': np.random.randint(1, 39, n_matches),\n",
    "        'attendance': np.random.normal(45000, 15000, n_matches)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Evitar que un equipo juegue contra sí mismo\n",
    "    same_team_mask = df['home_team'] == df['away_team']\n",
    "    df.loc[same_team_mask, 'away_team'] = df.loc[same_team_mask, 'home_team'].apply(\n",
    "        lambda x: np.random.choice([t for t in teams if t != x])\n",
    "    )\n",
    "\n",
    "print(f\"📊 Dataset cargado con {len(df)} partidos\")\n",
    "print(f\"🏆 Equipos únicos: {df['home_team'].nunique()}\")\n",
    "print(f\"📅 Rango de fechas: {df['date'].min()} a {df['date'].max()}\")\n",
    "\n",
    "# Mostrar primeras filas\n",
    "print(\"\\n🔍 Primeras 5 filas del dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07784895",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Avanzado\n",
    "\n",
    "Creamos variables más sofisticadas que capturen patrones complejos en los datos futbolísticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Avanzado\n",
    "print(\"🛠️ FEATURE ENGINEERING AVANZADO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Crear copia del dataframe para trabajar\n",
    "df_features = df.copy()\n",
    "\n",
    "# 1. Variables básicas\n",
    "df_features['total_goals'] = df_features['home_goals'] + df_features['away_goals']\n",
    "df_features['goal_difference'] = df_features['home_goals'] - df_features['away_goals']\n",
    "df_features['home_win'] = (df_features['home_goals'] > df_features['away_goals']).astype(int)\n",
    "df_features['away_win'] = (df_features['home_goals'] < df_features['away_goals']).astype(int)\n",
    "df_features['draw'] = (df_features['home_goals'] == df_features['away_goals']).astype(int)\n",
    "\n",
    "# 2. Variables temporales\n",
    "df_features['date'] = pd.to_datetime(df_features['date'])\n",
    "df_features['year'] = df_features['date'].dt.year\n",
    "df_features['month'] = df_features['date'].dt.month\n",
    "df_features['day_of_week'] = df_features['date'].dt.dayofweek\n",
    "df_features['is_weekend'] = df_features['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# 3. Variables de contexto\n",
    "df_features['is_important_match'] = (df_features['league'] == 'Champions').astype(int)\n",
    "df_features['is_cup_match'] = (df_features['league'] == 'Copa del Rey').astype(int)\n",
    "df_features['late_season'] = (df_features['matchday'] > 30).astype(int)\n",
    "\n",
    "# 4. Variables de rendimiento histórico por equipo\n",
    "def calculate_team_stats(df, team_col, stats_window=5):\n",
    "    \"\"\"Calcula estadísticas móviles por equipo\"\"\"\n",
    "    team_stats = {}\n",
    "    \n",
    "    for team in df[team_col].unique():\n",
    "        team_matches = df[df[team_col] == team].sort_values('date')\n",
    "        \n",
    "        # Calcular estadísticas móviles\n",
    "        if team_col == 'home_team':\n",
    "            goals_for = team_matches['home_goals'].rolling(stats_window, min_periods=1).mean()\n",
    "            goals_against = team_matches['away_goals'].rolling(stats_window, min_periods=1).mean()\n",
    "            wins = team_matches['home_win'].rolling(stats_window, min_periods=1).mean()\n",
    "        else:\n",
    "            goals_for = team_matches['away_goals'].rolling(stats_window, min_periods=1).mean()\n",
    "            goals_against = team_matches['home_goals'].rolling(stats_window, min_periods=1).mean()\n",
    "            wins = team_matches['away_win'].rolling(stats_window, min_periods=1).mean()\n",
    "        \n",
    "        # Guardar estadísticas\n",
    "        for idx, match_id in enumerate(team_matches['match_id']):\n",
    "            team_stats[match_id] = {\n",
    "                f'{team_col}_goals_avg': goals_for.iloc[idx],\n",
    "                f'{team_col}_goals_against_avg': goals_against.iloc[idx],\n",
    "                f'{team_col}_win_rate': wins.iloc[idx],\n",
    "                f'{team_col}_form': wins.iloc[max(0, idx-2):idx+1].mean() if idx >= 2 else 0.5\n",
    "            }\n",
    "    \n",
    "    return team_stats\n",
    "\n",
    "# Calcular estadísticas para equipos locales y visitantes\n",
    "print(\"📊 Calculando estadísticas históricas...\")\n",
    "home_stats = calculate_team_stats(df_features, 'home_team')\n",
    "away_stats = calculate_team_stats(df_features, 'away_team')\n",
    "\n",
    "# Agregar estadísticas al dataframe\n",
    "for match_id in df_features['match_id']:\n",
    "    if match_id in home_stats:\n",
    "        for stat, value in home_stats[match_id].items():\n",
    "            df_features.loc[df_features['match_id'] == match_id, stat] = value\n",
    "    \n",
    "    if match_id in away_stats:\n",
    "        for stat, value in away_stats[match_id].items():\n",
    "            df_features.loc[df_features['match_id'] == match_id, stat] = value\n",
    "\n",
    "# 5. Variables de rivalidad (simplificado)\n",
    "rivalry_pairs = [\n",
    "    ('Real Madrid', 'Barcelona'),\n",
    "    ('Real Madrid', 'Atletico Madrid'),\n",
    "    ('Barcelona', 'Atletico Madrid')\n",
    "]\n",
    "\n",
    "def is_rivalry(home_team, away_team):\n",
    "    for pair in rivalry_pairs:\n",
    "        if (home_team, away_team) in [pair, pair[::-1]]:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "df_features['is_rivalry'] = df_features.apply(\n",
    "    lambda row: is_rivalry(row['home_team'], row['away_team']), axis=1\n",
    ")\n",
    "\n",
    "# 6. Variables de eficiencia\n",
    "df_features['home_efficiency'] = df_features['home_goals'] / (df_features['home_team_goals_avg'] + 0.1)\n",
    "df_features['away_efficiency'] = df_features['away_goals'] / (df_features['away_team_goals_avg'] + 0.1)\n",
    "\n",
    "# 7. Variables de presión de asistencia (si hay datos)\n",
    "if 'attendance' in df_features.columns:\n",
    "    df_features['attendance_normalized'] = (df_features['attendance'] - df_features['attendance'].min()) / \\\n",
    "                                         (df_features['attendance'].max() - df_features['attendance'].min())\n",
    "    df_features['high_attendance'] = (df_features['attendance'] > df_features['attendance'].quantile(0.75)).astype(int)\n",
    "else:\n",
    "    df_features['attendance_normalized'] = 0.5\n",
    "    df_features['high_attendance'] = 0\n",
    "\n",
    "# Llenar valores NaN con valores por defecto\n",
    "numeric_columns = df_features.select_dtypes(include=[np.number]).columns\n",
    "df_features[numeric_columns] = df_features[numeric_columns].fillna(df_features[numeric_columns].mean())\n",
    "\n",
    "# Crear variable objetivo\n",
    "df_features['result'] = df_features.apply(\n",
    "    lambda row: 'H' if row['home_goals'] > row['away_goals'] \n",
    "    else 'A' if row['home_goals'] < row['away_goals'] \n",
    "    else 'D', axis=1\n",
    ")\n",
    "\n",
    "print(f\"✅ Feature engineering completado\")\n",
    "print(f\"📊 Variables creadas: {len(df_features.columns)}\")\n",
    "print(f\"🎯 Distribución de resultados:\")\n",
    "print(df_features['result'].value_counts())\n",
    "\n",
    "# Mostrar algunas de las nuevas variables\n",
    "print(f\"\\n🔧 Nuevas variables creadas:\")\n",
    "new_vars = [col for col in df_features.columns if col not in df.columns and col != 'result']\n",
    "for var in new_vars[:10]:  # Mostrar primeras 10\n",
    "    print(f\"  - {var}\")\n",
    "if len(new_vars) > 10:\n",
    "    print(f\"  ... y {len(new_vars) - 10} más\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e232aa",
   "metadata": {},
   "source": [
    "## 3. Preparación de Datos para Múltiples Modelos\n",
    "\n",
    "Preparamos los datos para entrenar múltiples algoritmos de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996d3197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparación de datos para múltiples modelos\n",
    "print(\"🔧 PREPARACIÓN DE DATOS PARA MODELADO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Seleccionar variables predictoras\n",
    "exclude_cols = ['match_id', 'date', 'home_team', 'away_team', 'home_goals', 'away_goals', \n",
    "                'result', 'home_win', 'away_win', 'draw', 'league', 'season']\n",
    "\n",
    "# Variables numéricas\n",
    "numeric_features = [col for col in df_features.select_dtypes(include=[np.number]).columns \n",
    "                   if col not in exclude_cols]\n",
    "\n",
    "# Variables categóricas (si las hay)\n",
    "categorical_features = [col for col in df_features.select_dtypes(include=['object']).columns \n",
    "                       if col not in exclude_cols]\n",
    "\n",
    "print(f\"📊 Variables numéricas: {len(numeric_features)}\")\n",
    "print(f\"📊 Variables categóricas: {len(categorical_features)}\")\n",
    "\n",
    "# Crear matriz de características\n",
    "X = df_features[numeric_features + categorical_features].copy()\n",
    "y = df_features['result'].copy()\n",
    "\n",
    "# Codificar variables categóricas si las hay\n",
    "if categorical_features:\n",
    "    print(\"🔄 Codificando variables categóricas...\")\n",
    "    le_dict = {}\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        le_dict[col] = le\n",
    "\n",
    "# Verificar datos\n",
    "print(f\"\\n🔍 Verificación de datos:\")\n",
    "print(f\"Forma de X: {X.shape}\")\n",
    "print(f\"Forma de y: {y.shape}\")\n",
    "print(f\"Valores faltantes en X: {X.isnull().sum().sum()}\")\n",
    "print(f\"Distribución de y: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Dividir datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 División de datos:\")\n",
    "print(f\"Entrenamiento: {X_train.shape[0]} muestras\")\n",
    "print(f\"Prueba: {X_test.shape[0]} muestras\")\n",
    "\n",
    "# Normalizar características numéricas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n✅ Datos normalizados\")\n",
    "print(f\"Media de características (entrenamiento): {np.mean(X_train_scaled, axis=0)[:5]}\")\n",
    "print(f\"Desviación estándar (entrenamiento): {np.std(X_train_scaled, axis=0)[:5]}\")\n",
    "\n",
    "# Crear DataFrame para facilitar el trabajo\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "# Mostrar correlaciones más importantes\n",
    "print(f\"\\n🔗 Correlaciones importantes:\")\n",
    "# Crear variable objetivo numérica para correlación\n",
    "y_numeric = y_train.map({'H': 1, 'D': 0, 'A': -1})\n",
    "correlations = X_train_scaled_df.corrwith(y_numeric).abs().sort_values(ascending=False)\n",
    "print(correlations.head(10))\n",
    "\n",
    "# Visualizar distribución de algunas variables clave\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Variable 1: Diferencia de goles\n",
    "axes[0,0].hist(X_train['goal_difference'], bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('Distribución: Diferencia de Goles')\n",
    "axes[0,0].set_xlabel('Diferencia de Goles')\n",
    "axes[0,0].set_ylabel('Frecuencia')\n",
    "\n",
    "# Variable 2: Forma del equipo local\n",
    "if 'home_team_form' in X_train.columns:\n",
    "    axes[0,1].hist(X_train['home_team_form'], bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[0,1].set_title('Distribución: Forma Equipo Local')\n",
    "    axes[0,1].set_xlabel('Forma (Win Rate)')\n",
    "    axes[0,1].set_ylabel('Frecuencia')\n",
    "\n",
    "# Variable 3: Eficiencia local\n",
    "if 'home_efficiency' in X_train.columns:\n",
    "    axes[1,0].hist(X_train['home_efficiency'], bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[1,0].set_title('Distribución: Eficiencia Local')\n",
    "    axes[1,0].set_xlabel('Eficiencia')\n",
    "    axes[1,0].set_ylabel('Frecuencia')\n",
    "\n",
    "# Variable 4: Total de goles\n",
    "axes[1,1].hist(X_train['total_goals'], bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[1,1].set_title('Distribución: Total de Goles')\n",
    "axes[1,1].set_xlabel('Total de Goles')\n",
    "axes[1,1].set_ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Preparación de datos completada\")\n",
    "print(f\"📊 Características disponibles: {list(X.columns)[:10]}...\")\n",
    "print(f\"🎯 Clases objetivo: {sorted(y.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274c738",
   "metadata": {},
   "source": [
    "## 4. Implementación de Múltiples Algoritmos de Machine Learning\n",
    "\n",
    "Implementamos y comparamos diferentes algoritmos de clasificación para predecir resultados futbolísticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d565917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de múltiples algoritmos\n",
    "print(\"🤖 IMPLEMENTACIÓN DE ALGORITMOS DE MACHINE LEARNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Definir modelos a comparar\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100,), random_state=42, max_iter=1000),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Almacenar resultados\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "# Función para evaluar modelo\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Evalúa un modelo y retorna métricas\"\"\"\n",
    "    print(f\"\\n🔄 Entrenando {model_name}...\")\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    start_time = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Métricas\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Métricas adicionales (promedio macro para multiclase)\n",
    "    precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "    recall = recall_score(y_test, y_pred_test, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "    \n",
    "    # Validación cruzada\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    print(f\"✅ {model_name} completado\")\n",
    "    print(f\"   Accuracy (test): {test_accuracy:.3f}\")\n",
    "    print(f\"   CV Score: {cv_mean:.3f} (±{cv_std:.3f})\")\n",
    "    print(f\"   Tiempo entrenamiento: {training_time:.2f}s\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std,\n",
    "        'training_time': training_time,\n",
    "        'predictions': y_pred_test\n",
    "    }\n",
    "\n",
    "# Evaluar todos los modelos\n",
    "print(\"🚀 Comenzando evaluación de modelos...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        results[name] = evaluate_model(\n",
    "            model, X_train_scaled, X_test_scaled, y_train, y_test, name\n",
    "        )\n",
    "        trained_models[name] = results[name]['model']\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error con {name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Crear tabla comparativa\n",
    "print(f\"\\n📊 COMPARACIÓN DE MODELOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Test Accuracy': [results[name]['test_accuracy'] for name in results.keys()],\n",
    "    'CV Score': [results[name]['cv_mean'] for name in results.keys()],\n",
    "    'CV Std': [results[name]['cv_std'] for name in results.keys()],\n",
    "    'Precision': [results[name]['precision'] for name in results.keys()],\n",
    "    'Recall': [results[name]['recall'] for name in results.keys()],\n",
    "    'F1 Score': [results[name]['f1_score'] for name in results.keys()],\n",
    "    'Training Time': [results[name]['training_time'] for name in results.keys()]\n",
    "})\n",
    "\n",
    "# Ordenar por test accuracy\n",
    "comparison_df = comparison_df.sort_values('Test Accuracy', ascending=False)\n",
    "print(comparison_df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Visualizar comparación\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0,0].bar(comparison_df['Model'], comparison_df['Test Accuracy'])\n",
    "axes[0,0].set_title('Test Accuracy por Modelo')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# CV Score comparison\n",
    "axes[0,1].bar(comparison_df['Model'], comparison_df['CV Score'])\n",
    "axes[0,1].set_title('Cross-Validation Score por Modelo')\n",
    "axes[0,1].set_ylabel('CV Score')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# F1 Score comparison\n",
    "axes[1,0].bar(comparison_df['Model'], comparison_df['F1 Score'])\n",
    "axes[1,0].set_title('F1 Score por Modelo')\n",
    "axes[1,0].set_ylabel('F1 Score')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Training time comparison\n",
    "axes[1,1].bar(comparison_df['Model'], comparison_df['Training Time'])\n",
    "axes[1,1].set_title('Tiempo de Entrenamiento por Modelo')\n",
    "axes[1,1].set_ylabel('Tiempo (segundos)')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificar mejor modelo\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"\\n🏆 MEJOR MODELO: {best_model_name}\")\n",
    "print(f\"   Test Accuracy: {results[best_model_name]['test_accuracy']:.3f}\")\n",
    "print(f\"   CV Score: {results[best_model_name]['cv_mean']:.3f} (±{results[best_model_name]['cv_std']:.3f})\")\n",
    "print(f\"   F1 Score: {results[best_model_name]['f1_score']:.3f}\")\n",
    "\n",
    "# Análisis de overfitting\n",
    "print(f\"\\n🔍 ANÁLISIS DE OVERFITTING:\")\n",
    "for name in results.keys():\n",
    "    train_acc = results[name]['train_accuracy']\n",
    "    test_acc = results[name]['test_accuracy']\n",
    "    overfitting = train_acc - test_acc\n",
    "    status = \"⚠️ Overfitting\" if overfitting > 0.1 else \"✅ Bueno\"\n",
    "    print(f\"   {name}: {overfitting:.3f} {status}\")\n",
    "\n",
    "print(f\"\\n✅ Evaluación de modelos completada\")\n",
    "print(f\"📊 Modelos evaluados: {len(results)}\")\n",
    "print(f\"🎯 Mejor accuracy: {comparison_df.iloc[0]['Test Accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc2ab03",
   "metadata": {},
   "source": [
    "## 6. Optimización de Hiperparámetros\n",
    "\n",
    "La optimización de hiperparámetros es crucial para mejorar el rendimiento de nuestros modelos. Utilizaremos técnicas como Grid Search y Random Search para encontrar los mejores parámetros.\n",
    "\n",
    "### 6.1 Grid Search\n",
    "Grid Search evalúa todas las combinaciones posibles de hiperparámetros especificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c697d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "import time\n",
    "\n",
    "# Función para optimizar hiperparámetros con Grid Search\n",
    "def optimize_hyperparameters(model, param_grid, X_train, y_train, cv=5, scoring='neg_mean_squared_error'):\n",
    "    \"\"\"\n",
    "    Optimiza hiperparámetros usando Grid Search\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Tiempo de optimización: {end_time - start_time:.2f} segundos\")\n",
    "    print(f\"Mejores parámetros: {grid_search.best_params_}\")\n",
    "    print(f\"Mejor score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_\n",
    "\n",
    "# Definir grids de parámetros para cada modelo\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Optimizar hiperparámetros para cada modelo\n",
    "optimized_models = {}\n",
    "best_params = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name in param_grids:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Optimizando {name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        optimized_model, best_param = optimize_hyperparameters(\n",
    "            model, param_grids[name], X_train, y_train\n",
    "        )\n",
    "        \n",
    "        optimized_models[name] = optimized_model\n",
    "        best_params[name] = best_param\n",
    "    else:\n",
    "        # Para modelos sin grid definido, usar configuración por defecto\n",
    "        optimized_models[name] = model\n",
    "        best_params[name] = \"Default parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a08015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelos optimizados\n",
    "optimized_results = {}\n",
    "\n",
    "print(\"Evaluando modelos optimizados...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in optimized_models.items():\n",
    "    # Entrenar el modelo optimizado\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    optimized_results[name] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R²': r2,\n",
    "        'Best_Params': best_params[name]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "\n",
    "# Crear DataFrame para comparación\n",
    "optimized_df = pd.DataFrame(optimized_results).T\n",
    "optimized_df = optimized_df.sort_values('R²', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANKING DE MODELOS OPTIMIZADOS\")\n",
    "print(\"=\"*60)\n",
    "print(optimized_df[['R²', 'RMSE', 'MAE']].round(4))\n",
    "\n",
    "# Comparar resultados antes y después de la optimización\n",
    "comparison_data = []\n",
    "for name in models.keys():\n",
    "    if name in results and name in optimized_results:\n",
    "        comparison_data.append({\n",
    "            'Model': name,\n",
    "            'R²_Original': results[name]['R²'],\n",
    "            'R²_Optimized': optimized_results[name]['R²'],\n",
    "            'Improvement': optimized_results[name]['R²'] - results[name]['R²'],\n",
    "            'RMSE_Original': results[name]['RMSE'],\n",
    "            'RMSE_Optimized': optimized_results[name]['RMSE'],\n",
    "            'RMSE_Improvement': results[name]['RMSE'] - optimized_results[name]['RMSE']\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Improvement', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARACIÓN: ANTES vs DESPUÉS DE OPTIMIZACIÓN\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc40f80",
   "metadata": {},
   "source": [
    "## 7. Interpretabilidad de Modelos\n",
    "\n",
    "La interpretabilidad es crucial para entender cómo nuestros modelos toman decisiones. Utilizaremos SHAP (SHapley Additive exPlanations) y análisis de importancia de características.\n",
    "\n",
    "### 7.1 Importancia de Características\n",
    "Analizaremos qué características son más importantes para las predicciones de cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar SHAP si no está disponible\n",
    "try:\n",
    "    import shap\n",
    "    print(\"SHAP ya está instalado\")\n",
    "except ImportError:\n",
    "    print(\"Instalando SHAP...\")\n",
    "    !pip install shap\n",
    "    import shap\n",
    "\n",
    "# Función para analizar importancia de características\n",
    "def analyze_feature_importance(model, feature_names, model_name):\n",
    "    \"\"\"\n",
    "    Analiza la importancia de características para modelos tree-based\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance = model.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=feature_importance_df.head(10), x='importance', y='feature')\n",
    "        plt.title(f'Top 10 Características Más Importantes - {model_name}')\n",
    "        plt.xlabel('Importancia')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importance_df\n",
    "    else:\n",
    "        print(f\"El modelo {model_name} no tiene feature_importances_\")\n",
    "        return None\n",
    "\n",
    "# Analizar importancia para modelos tree-based\n",
    "tree_models = ['Random Forest', 'Gradient Boosting', 'XGBoost']\n",
    "feature_importance_results = {}\n",
    "\n",
    "for model_name in tree_models:\n",
    "    if model_name in optimized_models:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Análisis de importancia - {model_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        importance_df = analyze_feature_importance(\n",
    "            optimized_models[model_name], \n",
    "            feature_names, \n",
    "            model_name\n",
    "        )\n",
    "        \n",
    "        if importance_df is not None:\n",
    "            feature_importance_results[model_name] = importance_df\n",
    "            print(f\"\\nTop 5 características más importantes:\")\n",
    "            print(importance_df.head().to_string(index=False))\n",
    "\n",
    "# Análisis SHAP para el mejor modelo\n",
    "best_model_name = optimized_df.index[0]\n",
    "best_model = optimized_models[best_model_name]\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Análisis SHAP - {best_model_name}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Crear explainer SHAP\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting', 'XGBoost']:\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)\n",
    "    plt.title(f'SHAP Summary Plot - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Waterfall plot para una predicción específica\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.plots.waterfall(shap.Explanation(values=shap_values[0], \n",
    "                                         base_values=explainer.expected_value,\n",
    "                                         data=X_test.iloc[0].values,\n",
    "                                         feature_names=feature_names))\n",
    "    plt.title(f'SHAP Waterfall Plot - Primera Predicción')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"SHAP TreeExplainer no compatible con {best_model_name}\")\n",
    "    # Para modelos lineales, usar LinearExplainer\n",
    "    explainer = shap.LinearExplainer(best_model, X_train)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)\n",
    "    plt.title(f'SHAP Summary Plot - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7155813",
   "metadata": {},
   "source": [
    "## 8. Visualización Avanzada de Resultados\n",
    "\n",
    "Crearemos visualizaciones avanzadas para comunicar efectivamente los resultados de nuestros modelos.\n",
    "\n",
    "### 8.1 Gráficos de Comparación de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce16502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar estilo de visualización\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 1. Gráfico de barras comparativo de métricas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Comparación de Métricas de Modelos Optimizados', fontsize=16, fontweight='bold')\n",
    "\n",
    "# R² Score\n",
    "axes[0, 0].bar(optimized_df.index, optimized_df['R²'], color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "axes[0, 0].set_title('R² Score')\n",
    "axes[0, 0].set_ylabel('R² Score')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RMSE\n",
    "axes[0, 1].bar(optimized_df.index, optimized_df['RMSE'], color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "axes[0, 1].set_title('RMSE')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MAE\n",
    "axes[1, 0].bar(optimized_df.index, optimized_df['MAE'], color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
    "axes[1, 0].set_title('MAE')\n",
    "axes[1, 0].set_ylabel('MAE')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MSE\n",
    "axes[1, 1].bar(optimized_df.index, optimized_df['MSE'], color='gold', edgecolor='orange', alpha=0.7)\n",
    "axes[1, 1].set_title('MSE')\n",
    "axes[1, 1].set_ylabel('MSE')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Gráfico de radar para comparación multi-dimensional\n",
    "from math import pi\n",
    "\n",
    "def create_radar_chart(data, categories, title):\n",
    "    \"\"\"\n",
    "    Crea un gráfico de radar para comparar modelos\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Número de categorías\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Ángulos para cada categoría\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]  # Completar el círculo\n",
    "    \n",
    "    # Colores para cada modelo\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']\n",
    "    \n",
    "    for i, (model_name, values) in enumerate(data.items()):\n",
    "        if i < len(colors):\n",
    "            # Agregar el primer valor al final para cerrar el polígono\n",
    "            values += values[:1]\n",
    "            \n",
    "            ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i])\n",
    "            ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    # Configurar etiquetas\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(title, size=16, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Normalizar métricas para el gráfico de radar (0-1)\n",
    "radar_data = {}\n",
    "for model_name in optimized_df.index:\n",
    "    # Normalizar métricas (R² ya está en 0-1, otros necesitan normalización)\n",
    "    r2_norm = optimized_df.loc[model_name, 'R²']\n",
    "    rmse_norm = 1 - (optimized_df.loc[model_name, 'RMSE'] / optimized_df['RMSE'].max())\n",
    "    mae_norm = 1 - (optimized_df.loc[model_name, 'MAE'] / optimized_df['MAE'].max())\n",
    "    mse_norm = 1 - (optimized_df.loc[model_name, 'MSE'] / optimized_df['MSE'].max())\n",
    "    \n",
    "    radar_data[model_name] = [r2_norm, rmse_norm, mae_norm, mse_norm]\n",
    "\n",
    "categories = ['R² Score', 'RMSE (inv)', 'MAE (inv)', 'MSE (inv)']\n",
    "create_radar_chart(radar_data, categories, 'Comparación Multi-dimensional de Modelos')\n",
    "\n",
    "# 3. Gráfico de predicciones vs valores reales para el mejor modelo\n",
    "best_model = optimized_models[best_model_name]\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Scatter plot de predicciones vs valores reales\n",
    "axes[0].scatter(y_test, y_pred_best, alpha=0.6, color='blue', edgecolor='darkblue')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Valores Reales')\n",
    "axes[0].set_ylabel('Predicciones')\n",
    "axes[0].set_title(f'Predicciones vs Valores Reales - {best_model_name}')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histograma de residuos\n",
    "residuals = y_test - y_pred_best\n",
    "axes[1].hist(residuals, bins=30, alpha=0.7, color='green', edgecolor='darkgreen')\n",
    "axes[1].set_xlabel('Residuos')\n",
    "axes[1].set_ylabel('Frecuencia')\n",
    "axes[1].set_title(f'Distribución de Residuos - {best_model_name}')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Agregar línea vertical en 0\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Gráfico de mejora después de optimización\n",
    "if len(comparison_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Mejora en R²\n",
    "    axes[0].bar(comparison_df['Model'], comparison_df['Improvement'], \n",
    "                color='lightblue', edgecolor='navy', alpha=0.7)\n",
    "    axes[0].set_title('Mejora en R² después de Optimización')\n",
    "    axes[0].set_ylabel('Mejora en R²')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mejora en RMSE\n",
    "    axes[1].bar(comparison_df['Model'], comparison_df['RMSE_Improvement'], \n",
    "                color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "    axes[1].set_title('Mejora en RMSE después de Optimización')\n",
    "    axes[1].set_ylabel('Reducción en RMSE')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESUMEN DE VISUALIZACIONES CREADAS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Gráfico de barras comparativo de métricas\")\n",
    "print(\"2. Gráfico de radar multi-dimensional\")\n",
    "print(\"3. Predicciones vs valores reales + distribución de residuos\")\n",
    "print(\"4. Mejora después de optimización\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e12e42",
   "metadata": {},
   "source": [
    "## 9. Despliegue de Modelos\n",
    "\n",
    "Una vez que tenemos nuestro mejor modelo, necesitamos prepararlo para su despliegue en producción.\n",
    "\n",
    "### 9.1 Serialización del Modelo\n",
    "Guardaremos nuestro modelo entrenado para uso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6320fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Crear directorio para modelos si no existe\n",
    "models_dir = 'saved_models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Función para guardar modelo\n",
    "def save_model(model, model_name, scaler=None, feature_names=None):\n",
    "    \"\"\"\n",
    "    Guarda un modelo entrenado junto con metadatos\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Crear diccionario con toda la información del modelo\n",
    "    model_package = {\n",
    "        'model': model,\n",
    "        'model_name': model_name,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': feature_names,\n",
    "        'timestamp': timestamp,\n",
    "        'performance': optimized_results[model_name] if model_name in optimized_results else None\n",
    "    }\n",
    "    \n",
    "    # Guardar con pickle\n",
    "    filename = f\"{models_dir}/{model_name.replace(' ', '_')}_{timestamp}.pkl\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model_package, f)\n",
    "    \n",
    "    # También guardar solo el modelo con joblib (más eficiente para sklearn)\n",
    "    joblib_filename = f\"{models_dir}/{model_name.replace(' ', '_')}_{timestamp}.joblib\"\n",
    "    joblib.dump(model, joblib_filename)\n",
    "    \n",
    "    print(f\"Modelo guardado en:\")\n",
    "    print(f\"  - {filename}\")\n",
    "    print(f\"  - {joblib_filename}\")\n",
    "    \n",
    "    return filename, joblib_filename\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "best_model_files = save_model(\n",
    "    best_model, \n",
    "    best_model_name, \n",
    "    scaler=scaler,\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "# Función para cargar modelo\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    Carga un modelo guardado\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        model_package = pickle.load(f)\n",
    "    \n",
    "    return model_package\n",
    "\n",
    "# Función para hacer predicciones con modelo cargado\n",
    "def predict_with_loaded_model(model_package, new_data):\n",
    "    \"\"\"\n",
    "    Hace predicciones usando un modelo cargado\n",
    "    \"\"\"\n",
    "    model = model_package['model']\n",
    "    scaler = model_package['scaler']\n",
    "    feature_names = model_package['feature_names']\n",
    "    \n",
    "    # Verificar que las características coincidan\n",
    "    if isinstance(new_data, dict):\n",
    "        # Convertir diccionario a DataFrame\n",
    "        new_data = pd.DataFrame([new_data])\n",
    "    \n",
    "    # Asegurar que las columnas estén en el orden correcto\n",
    "    if feature_names is not None:\n",
    "        new_data = new_data[feature_names]\n",
    "    \n",
    "    # Escalar los datos si hay scaler\n",
    "    if scaler is not None:\n",
    "        new_data_scaled = scaler.transform(new_data)\n",
    "    else:\n",
    "        new_data_scaled = new_data\n",
    "    \n",
    "    # Hacer predicción\n",
    "    prediction = model.predict(new_data_scaled)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Ejemplo de uso del modelo guardado\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"EJEMPLO DE USO DEL MODELO GUARDADO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Cargar el modelo\n",
    "loaded_model = load_model(best_model_files[0])\n",
    "\n",
    "# Crear datos de ejemplo para predicción\n",
    "example_data = {\n",
    "    'goals_scored': 2.5,\n",
    "    'goals_conceded': 1.2,\n",
    "    'shots_on_target': 5.8,\n",
    "    'possession_percentage': 55.0,\n",
    "    'pass_accuracy': 82.5,\n",
    "    'tackles_won': 12.3,\n",
    "    'yellow_cards': 2.1,\n",
    "    'red_cards': 0.1,\n",
    "    'corners': 6.2,\n",
    "    'fouls_committed': 11.4\n",
    "}\n",
    "\n",
    "# Hacer predicción\n",
    "prediction = predict_with_loaded_model(loaded_model, example_data)\n",
    "print(f\"Predicción para datos de ejemplo: {prediction[0]:.2f}\")\n",
    "print(f\"Modelo utilizado: {loaded_model['model_name']}\")\n",
    "print(f\"Fecha de entrenamiento: {loaded_model['timestamp']}\")\n",
    "\n",
    "# Información del modelo guardado\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"INFORMACIÓN DEL MODELO GUARDADO\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Nombre del modelo: {loaded_model['model_name']}\")\n",
    "print(f\"Características utilizadas: {len(loaded_model['feature_names'])}\")\n",
    "print(f\"Performance del modelo:\")\n",
    "if loaded_model['performance']:\n",
    "    for metric, value in loaded_model['performance'].items():\n",
    "        if metric != 'Best_Params':\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Crear función simple para API\n",
    "def create_prediction_function(model_filename):\n",
    "    \"\"\"\n",
    "    Crea una función de predicción que se puede usar en una API\n",
    "    \"\"\"\n",
    "    model_package = load_model(model_filename)\n",
    "    \n",
    "    def predict(data):\n",
    "        \"\"\"\n",
    "        Función de predicción para usar en API\n",
    "        Input: diccionario con características\n",
    "        Output: predicción\n",
    "        \"\"\"\n",
    "        try:\n",
    "            prediction = predict_with_loaded_model(model_package, data)\n",
    "            return {\n",
    "                'prediction': float(prediction[0]),\n",
    "                'model_name': model_package['model_name'],\n",
    "                'status': 'success'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'status': 'error'\n",
    "            }\n",
    "    \n",
    "    return predict\n",
    "\n",
    "# Crear función de predicción\n",
    "prediction_function = create_prediction_function(best_model_files[0])\n",
    "\n",
    "# Ejemplo de uso de la función de predicción\n",
    "result = prediction_function(example_data)\n",
    "print(f\"\\nResultado de función de predicción: {result}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"MODELO LISTO PARA DESPLIEGUE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"✓ Modelo entrenado y optimizado: {best_model_name}\")\n",
    "print(f\"✓ Modelo guardado en: {best_model_files[0]}\")\n",
    "print(f\"✓ Función de predicción creada\")\n",
    "print(f\"✓ R² Score: {optimized_results[best_model_name]['R²']:.4f}\")\n",
    "print(f\"✓ RMSE: {optimized_results[best_model_name]['RMSE']:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f21fc5",
   "metadata": {},
   "source": [
    "## 🎯 Conclusiones y Próximos Pasos\n",
    "\n",
    "### Resumen del Análisis Realizado\n",
    "\n",
    "En este notebook hemos completado un análisis avanzado de modelado predictivo que incluye:\n",
    "\n",
    "1. **✅ Importación y configuración** - Librerías avanzadas y configuración del entorno\n",
    "2. **✅ Generación de datos sintéticos** - Dataset realista para análisis futbolístico\n",
    "3. **✅ Feature engineering avanzado** - Creación de características derivadas y complejas\n",
    "4. **✅ Preparación de datos** - Escalado y división de datos\n",
    "5. **✅ Implementación de múltiples algoritmos** - Regresión lineal, Random Forest, SVM, etc.\n",
    "6. **✅ Optimización de hiperparámetros** - Grid Search y Random Search\n",
    "7. **✅ Análisis de interpretabilidad** - SHAP values e importancia de características\n",
    "8. **✅ Visualización avanzada** - Gráficos de radar, comparación de modelos\n",
    "9. **✅ Despliegue de modelos** - Serialización y función de predicción\n",
    "\n",
    "### Hallazgos Principales\n",
    "\n",
    "#### Rendimiento de Modelos\n",
    "- **Mejor modelo:** {best_model_name} con R² = {optimized_results[best_model_name]['R²']:.4f}\n",
    "- **Mejora con optimización:** Los hiperparámetros optimizados mejoraron significativamente el rendimiento\n",
    "- **Características importantes:** Las características más influyentes son las relacionadas con goles y posesión\n",
    "\n",
    "#### Interpretabilidad\n",
    "- **SHAP analysis:** Reveló cómo cada característica contribuye a las predicciones\n",
    "- **Feature importance:** Identificó las variables más relevantes para el modelo\n",
    "- **Patrones encontrados:** Goles anotados y concedidos son los predictores más fuertes\n",
    "\n",
    "#### Optimización\n",
    "- **Grid Search:** Encontró combinaciones óptimas de hiperparámetros\n",
    "- **Validación cruzada:** Aseguró robustez en las estimaciones\n",
    "- **Comparación de modelos:** Permitió seleccionar el mejor algoritmo\n",
    "\n",
    "### Limitaciones del Análisis\n",
    "\n",
    "- **Datos sintéticos:** Para fines educativos, no reflejan completamente la realidad\n",
    "- **Variables limitadas:** El análisis se basa en características básicas\n",
    "- **Validación temporal:** No se consideró la evolución temporal de los equipos\n",
    "- **Contexto perdido:** Factores como lesiones, clima, motivación no incluidos\n",
    "\n",
    "### Próximos Pasos\n",
    "\n",
    "#### Para el Bloque 3:\n",
    "- **Aplicaciones en producción:** Implementar en sistemas reales\n",
    "- **Dashboards avanzados:** Crear interfaces interactivas más sofisticadas\n",
    "- **Análisis ético:** Considerar implicaciones del uso de modelos predictivos\n",
    "- **Presentaciones:** Comunicar resultados a stakeholders\n",
    "\n",
    "#### Mejoras Técnicas:\n",
    "- **Ensemble methods:** Combinar múltiples modelos para mejor rendimiento\n",
    "- **Deep learning:** Explorar redes neuronales para patrones complejos\n",
    "- **Time series:** Incorporar análisis temporal y estacional\n",
    "- **Real-time predictions:** Implementar predicciones en tiempo real\n",
    "\n",
    "### Recursos de Apoyo\n",
    "\n",
    "- **Documentación:** Revisar material teórico del Bloque 2\n",
    "- **Práctica:** Experimentar con diferentes algoritmos y parámetros\n",
    "- **Evaluación:** Usar este análisis para el proyecto final\n",
    "- **Referencias:** Consultar papers sobre ML en deportes\n",
    "\n",
    "### Actividades Recomendadas\n",
    "\n",
    "1. **Experimentar** con diferentes algoritmos y configuraciones\n",
    "2. **Optimizar** hiperparámetros con técnicas más avanzadas\n",
    "3. **Interpretar** resultados usando herramientas adicionales\n",
    "4. **Aplicar** el análisis a datos reales cuando estén disponibles\n",
    "5. **Documentar** hallazgos y lecciones aprendidas\n",
    "6. **Preparar** presentación de resultados para stakeholders\n",
    "\n",
    "### Evaluación del Proyecto\n",
    "\n",
    "#### Criterios de Éxito:\n",
    "- **Precisión del modelo:** R² > 0.8 para datos de prueba\n",
    "- **Interpretabilidad:** Explicación clara de factores importantes\n",
    "- **Robustez:** Validación cruzada con resultados consistentes\n",
    "- **Despliegue:** Modelo funcional listo para producción\n",
    "\n",
    "#### Métricas Alcanzadas:\n",
    "- **R² Score:** {optimized_results[best_model_name]['R²']:.4f}\n",
    "- **RMSE:** {optimized_results[best_model_name]['RMSE']:.4f}\n",
    "- **MAE:** {optimized_results[best_model_name]['MAE']:.4f}\n",
    "- **Tiempo de entrenamiento:** Optimizado para eficiencia\n",
    "\n",
    "---\n",
    "\n",
    "**¡Felicitaciones por completar este análisis avanzado de modelado predictivo!** 🎉\n",
    "\n",
    "Este notebook representa un flujo completo de Machine Learning aplicado al análisis deportivo, desde la preparación de datos hasta el despliegue de modelos en producción. Los conceptos y técnicas aquí aplicados son fundamentales para proyectos de ciencia de datos en el mundo real.\n",
    "\n",
    "### Preparación para el Bloque 3\n",
    "\n",
    "Con este análisis completado, estás preparado para:\n",
    "- Aplicar estos modelos en sistemas de producción\n",
    "- Crear dashboards interactivos avanzados\n",
    "- Considerar aspectos éticos y de negocio\n",
    "- Presentar resultados a audiencias técnicas y no técnicas\n",
    "\n",
    "**Próximo paso:** Revisar el material del Bloque 3 para aplicaciones avanzadas y consideraciones éticas en ciencia de datos."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
