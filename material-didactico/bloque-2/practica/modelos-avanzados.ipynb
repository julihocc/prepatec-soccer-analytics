{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed11b6e",
   "metadata": {},
   "source": [
    "# Modelos Avanzados de Machine Learning para An√°lisis Futbol√≠stico\n",
    "\n",
    "## Objetivos del Notebook\n",
    "\n",
    "Este notebook implementa t√©cnicas avanzadas de machine learning para el an√°lisis predictivo en f√∫tbol, utilizando el dataset \"champs\" y expandiendo el an√°lisis del Bloque 1 con modelos m√°s sofisticados.\n",
    "\n",
    "### Contenido del Bloque 2:\n",
    "\n",
    "1. **Feature Engineering Avanzado**\n",
    "2. **Implementaci√≥n de M√∫ltiples Algoritmos**\n",
    "3. **Evaluaci√≥n Comparativa de Modelos**\n",
    "4. **Optimizaci√≥n de Hiperpar√°metros**\n",
    "5. **An√°lisis de Importancia de Variables**\n",
    "6. **Validaci√≥n Cruzada y Robustez**\n",
    "7. **Interpretaci√≥n de Resultados**\n",
    "8. **Despliegue de Modelos**\n",
    "\n",
    "### Algoritmos a Implementar:\n",
    "- **Regresi√≥n Log√≠stica** (baseline)\n",
    "- **Random Forest** (ensemble method)\n",
    "- **Support Vector Machine** (SVM)\n",
    "- **Gradient Boosting** (XGBoost)\n",
    "- **Redes Neuronales** (MLPClassifier)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc36e5",
   "metadata": {},
   "source": [
    "## 1. Importar Librer√≠as y Configuraci√≥n Inicial\n",
    "\n",
    "Importamos las librer√≠as necesarias para el an√°lisis avanzado de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b97126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as b√°sicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librer√≠as de machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                           confusion_matrix, classification_report, roc_auc_score, roc_curve)\n",
    "\n",
    "# Algoritmos de clasificaci√≥n\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Librer√≠as adicionales\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Todas las librer√≠as importadas exitosamente\")\n",
    "print(f\"üìä Versi√≥n de scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"üêç Versi√≥n de Python: {sys.version}\")\n",
    "print(f\"üìÖ Fecha de ejecuci√≥n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c248be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos desde el Bloque 1 o generar datos de ejemplo\n",
    "print(\"üîÑ Cargando dataset...\")\n",
    "\n",
    "try:\n",
    "    # Intentar cargar el dataset real\n",
    "    df = pd.read_csv('../recursos/champs.csv')\n",
    "    print(\"‚úÖ Dataset 'champs' cargado desde archivo\")\n",
    "except FileNotFoundError:\n",
    "    # Generar dataset de ejemplo m√°s completo para demostraci√≥n\n",
    "    print(\"üìù Generando dataset de ejemplo...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_matches = 1000\n",
    "    teams = ['Real Madrid', 'Barcelona', 'Atletico Madrid', 'Valencia', 'Sevilla', \n",
    "             'Athletic Bilbao', 'Real Sociedad', 'Villarreal', 'Betis', 'Getafe']\n",
    "    \n",
    "    # Generar datos base\n",
    "    data = {\n",
    "        'match_id': range(1, n_matches + 1),\n",
    "        'date': pd.date_range('2022-01-01', periods=n_matches, freq='D'),\n",
    "        'home_team': np.random.choice(teams, n_matches),\n",
    "        'away_team': np.random.choice(teams, n_matches),\n",
    "        'home_goals': np.random.poisson(1.3, n_matches),\n",
    "        'away_goals': np.random.poisson(1.1, n_matches),\n",
    "        'league': np.random.choice(['La Liga', 'Copa del Rey', 'Champions'], n_matches, p=[0.6, 0.2, 0.2]),\n",
    "        'season': np.random.choice(['2022-23', '2023-24'], n_matches),\n",
    "        'matchday': np.random.randint(1, 39, n_matches),\n",
    "        'attendance': np.random.normal(45000, 15000, n_matches)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Evitar que un equipo juegue contra s√≠ mismo\n",
    "    same_team_mask = df['home_team'] == df['away_team']\n",
    "    df.loc[same_team_mask, 'away_team'] = df.loc[same_team_mask, 'home_team'].apply(\n",
    "        lambda x: np.random.choice([t for t in teams if t != x])\n",
    "    )\n",
    "\n",
    "print(f\"üìä Dataset cargado con {len(df)} partidos\")\n",
    "print(f\"üèÜ Equipos √∫nicos: {df['home_team'].nunique()}\")\n",
    "print(f\"üìÖ Rango de fechas: {df['date'].min()} a {df['date'].max()}\")\n",
    "\n",
    "# Mostrar primeras filas\n",
    "print(\"\\nüîç Primeras 5 filas del dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07784895",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Avanzado\n",
    "\n",
    "Creamos variables m√°s sofisticadas que capturen patrones complejos en los datos futbol√≠sticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Avanzado\n",
    "print(\"üõ†Ô∏è FEATURE ENGINEERING AVANZADO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Crear copia del dataframe para trabajar\n",
    "df_features = df.copy()\n",
    "\n",
    "# 1. Variables b√°sicas\n",
    "df_features['total_goals'] = df_features['home_goals'] + df_features['away_goals']\n",
    "df_features['goal_difference'] = df_features['home_goals'] - df_features['away_goals']\n",
    "df_features['home_win'] = (df_features['home_goals'] > df_features['away_goals']).astype(int)\n",
    "df_features['away_win'] = (df_features['home_goals'] < df_features['away_goals']).astype(int)\n",
    "df_features['draw'] = (df_features['home_goals'] == df_features['away_goals']).astype(int)\n",
    "\n",
    "# 2. Variables temporales\n",
    "df_features['date'] = pd.to_datetime(df_features['date'])\n",
    "df_features['year'] = df_features['date'].dt.year\n",
    "df_features['month'] = df_features['date'].dt.month\n",
    "df_features['day_of_week'] = df_features['date'].dt.dayofweek\n",
    "df_features['is_weekend'] = df_features['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# 3. Variables de contexto\n",
    "df_features['is_important_match'] = (df_features['league'] == 'Champions').astype(int)\n",
    "df_features['is_cup_match'] = (df_features['league'] == 'Copa del Rey').astype(int)\n",
    "df_features['late_season'] = (df_features['matchday'] > 30).astype(int)\n",
    "\n",
    "# 4. Variables de rendimiento hist√≥rico por equipo\n",
    "def calculate_team_stats(df, team_col, stats_window=5):\n",
    "    \"\"\"Calcula estad√≠sticas m√≥viles por equipo\"\"\"\n",
    "    team_stats = {}\n",
    "    \n",
    "    for team in df[team_col].unique():\n",
    "        team_matches = df[df[team_col] == team].sort_values('date')\n",
    "        \n",
    "        # Calcular estad√≠sticas m√≥viles\n",
    "        if team_col == 'home_team':\n",
    "            goals_for = team_matches['home_goals'].rolling(stats_window, min_periods=1).mean()\n",
    "            goals_against = team_matches['away_goals'].rolling(stats_window, min_periods=1).mean()\n",
    "            wins = team_matches['home_win'].rolling(stats_window, min_periods=1).mean()\n",
    "        else:\n",
    "            goals_for = team_matches['away_goals'].rolling(stats_window, min_periods=1).mean()\n",
    "            goals_against = team_matches['home_goals'].rolling(stats_window, min_periods=1).mean()\n",
    "            wins = team_matches['away_win'].rolling(stats_window, min_periods=1).mean()\n",
    "        \n",
    "        # Guardar estad√≠sticas\n",
    "        for idx, match_id in enumerate(team_matches['match_id']):\n",
    "            team_stats[match_id] = {\n",
    "                f'{team_col}_goals_avg': goals_for.iloc[idx],\n",
    "                f'{team_col}_goals_against_avg': goals_against.iloc[idx],\n",
    "                f'{team_col}_win_rate': wins.iloc[idx],\n",
    "                f'{team_col}_form': wins.iloc[max(0, idx-2):idx+1].mean() if idx >= 2 else 0.5\n",
    "            }\n",
    "    \n",
    "    return team_stats\n",
    "\n",
    "# Calcular estad√≠sticas para equipos locales y visitantes\n",
    "print(\"üìä Calculando estad√≠sticas hist√≥ricas...\")\n",
    "home_stats = calculate_team_stats(df_features, 'home_team')\n",
    "away_stats = calculate_team_stats(df_features, 'away_team')\n",
    "\n",
    "# Agregar estad√≠sticas al dataframe\n",
    "for match_id in df_features['match_id']:\n",
    "    if match_id in home_stats:\n",
    "        for stat, value in home_stats[match_id].items():\n",
    "            df_features.loc[df_features['match_id'] == match_id, stat] = value\n",
    "    \n",
    "    if match_id in away_stats:\n",
    "        for stat, value in away_stats[match_id].items():\n",
    "            df_features.loc[df_features['match_id'] == match_id, stat] = value\n",
    "\n",
    "# 5. Variables de rivalidad (simplificado)\n",
    "rivalry_pairs = [\n",
    "    ('Real Madrid', 'Barcelona'),\n",
    "    ('Real Madrid', 'Atletico Madrid'),\n",
    "    ('Barcelona', 'Atletico Madrid')\n",
    "]\n",
    "\n",
    "def is_rivalry(home_team, away_team):\n",
    "    for pair in rivalry_pairs:\n",
    "        if (home_team, away_team) in [pair, pair[::-1]]:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "df_features['is_rivalry'] = df_features.apply(\n",
    "    lambda row: is_rivalry(row['home_team'], row['away_team']), axis=1\n",
    ")\n",
    "\n",
    "# 6. Variables de eficiencia\n",
    "df_features['home_efficiency'] = df_features['home_goals'] / (df_features['home_team_goals_avg'] + 0.1)\n",
    "df_features['away_efficiency'] = df_features['away_goals'] / (df_features['away_team_goals_avg'] + 0.1)\n",
    "\n",
    "# 7. Variables de presi√≥n de asistencia (si hay datos)\n",
    "if 'attendance' in df_features.columns:\n",
    "    df_features['attendance_normalized'] = (df_features['attendance'] - df_features['attendance'].min()) / \\\n",
    "                                         (df_features['attendance'].max() - df_features['attendance'].min())\n",
    "    df_features['high_attendance'] = (df_features['attendance'] > df_features['attendance'].quantile(0.75)).astype(int)\n",
    "else:\n",
    "    df_features['attendance_normalized'] = 0.5\n",
    "    df_features['high_attendance'] = 0\n",
    "\n",
    "# Llenar valores NaN con valores por defecto\n",
    "numeric_columns = df_features.select_dtypes(include=[np.number]).columns\n",
    "df_features[numeric_columns] = df_features[numeric_columns].fillna(df_features[numeric_columns].mean())\n",
    "\n",
    "# Crear variable objetivo\n",
    "df_features['result'] = df_features.apply(\n",
    "    lambda row: 'H' if row['home_goals'] > row['away_goals'] \n",
    "    else 'A' if row['home_goals'] < row['away_goals'] \n",
    "    else 'D', axis=1\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Feature engineering completado\")\n",
    "print(f\"üìä Variables creadas: {len(df_features.columns)}\")\n",
    "print(f\"üéØ Distribuci√≥n de resultados:\")\n",
    "print(df_features['result'].value_counts())\n",
    "\n",
    "# Mostrar algunas de las nuevas variables\n",
    "print(f\"\\nüîß Nuevas variables creadas:\")\n",
    "new_vars = [col for col in df_features.columns if col not in df.columns and col != 'result']\n",
    "for var in new_vars[:10]:  # Mostrar primeras 10\n",
    "    print(f\"  - {var}\")\n",
    "if len(new_vars) > 10:\n",
    "    print(f\"  ... y {len(new_vars) - 10} m√°s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e232aa",
   "metadata": {},
   "source": [
    "## 3. Preparaci√≥n de Datos para M√∫ltiples Modelos\n",
    "\n",
    "Preparamos los datos para entrenar m√∫ltiples algoritmos de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996d3197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparaci√≥n de datos para m√∫ltiples modelos\n",
    "print(\"üîß PREPARACI√ìN DE DATOS PARA MODELADO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Seleccionar variables predictoras\n",
    "exclude_cols = ['match_id', 'date', 'home_team', 'away_team', 'home_goals', 'away_goals', \n",
    "                'result', 'home_win', 'away_win', 'draw', 'league', 'season']\n",
    "\n",
    "# Variables num√©ricas\n",
    "numeric_features = [col for col in df_features.select_dtypes(include=[np.number]).columns \n",
    "                   if col not in exclude_cols]\n",
    "\n",
    "# Variables categ√≥ricas (si las hay)\n",
    "categorical_features = [col for col in df_features.select_dtypes(include=['object']).columns \n",
    "                       if col not in exclude_cols]\n",
    "\n",
    "print(f\"üìä Variables num√©ricas: {len(numeric_features)}\")\n",
    "print(f\"üìä Variables categ√≥ricas: {len(categorical_features)}\")\n",
    "\n",
    "# Crear matriz de caracter√≠sticas\n",
    "X = df_features[numeric_features + categorical_features].copy()\n",
    "y = df_features['result'].copy()\n",
    "\n",
    "# Codificar variables categ√≥ricas si las hay\n",
    "if categorical_features:\n",
    "    print(\"üîÑ Codificando variables categ√≥ricas...\")\n",
    "    le_dict = {}\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        le_dict[col] = le\n",
    "\n",
    "# Verificar datos\n",
    "print(f\"\\nüîç Verificaci√≥n de datos:\")\n",
    "print(f\"Forma de X: {X.shape}\")\n",
    "print(f\"Forma de y: {y.shape}\")\n",
    "print(f\"Valores faltantes en X: {X.isnull().sum().sum()}\")\n",
    "print(f\"Distribuci√≥n de y: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Dividir datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Divisi√≥n de datos:\")\n",
    "print(f\"Entrenamiento: {X_train.shape[0]} muestras\")\n",
    "print(f\"Prueba: {X_test.shape[0]} muestras\")\n",
    "\n",
    "# Normalizar caracter√≠sticas num√©ricas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n‚úÖ Datos normalizados\")\n",
    "print(f\"Media de caracter√≠sticas (entrenamiento): {np.mean(X_train_scaled, axis=0)[:5]}\")\n",
    "print(f\"Desviaci√≥n est√°ndar (entrenamiento): {np.std(X_train_scaled, axis=0)[:5]}\")\n",
    "\n",
    "# Crear DataFrame para facilitar el trabajo\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "# Mostrar correlaciones m√°s importantes\n",
    "print(f\"\\nüîó Correlaciones importantes:\")\n",
    "# Crear variable objetivo num√©rica para correlaci√≥n\n",
    "y_numeric = y_train.map({'H': 1, 'D': 0, 'A': -1})\n",
    "correlations = X_train_scaled_df.corrwith(y_numeric).abs().sort_values(ascending=False)\n",
    "print(correlations.head(10))\n",
    "\n",
    "# Visualizar distribuci√≥n de algunas variables clave\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Variable 1: Diferencia de goles\n",
    "axes[0,0].hist(X_train['goal_difference'], bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('Distribuci√≥n: Diferencia de Goles')\n",
    "axes[0,0].set_xlabel('Diferencia de Goles')\n",
    "axes[0,0].set_ylabel('Frecuencia')\n",
    "\n",
    "# Variable 2: Forma del equipo local\n",
    "if 'home_team_form' in X_train.columns:\n",
    "    axes[0,1].hist(X_train['home_team_form'], bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[0,1].set_title('Distribuci√≥n: Forma Equipo Local')\n",
    "    axes[0,1].set_xlabel('Forma (Win Rate)')\n",
    "    axes[0,1].set_ylabel('Frecuencia')\n",
    "\n",
    "# Variable 3: Eficiencia local\n",
    "if 'home_efficiency' in X_train.columns:\n",
    "    axes[1,0].hist(X_train['home_efficiency'], bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[1,0].set_title('Distribuci√≥n: Eficiencia Local')\n",
    "    axes[1,0].set_xlabel('Eficiencia')\n",
    "    axes[1,0].set_ylabel('Frecuencia')\n",
    "\n",
    "# Variable 4: Total de goles\n",
    "axes[1,1].hist(X_train['total_goals'], bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[1,1].set_title('Distribuci√≥n: Total de Goles')\n",
    "axes[1,1].set_xlabel('Total de Goles')\n",
    "axes[1,1].set_ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Preparaci√≥n de datos completada\")\n",
    "print(f\"üìä Caracter√≠sticas disponibles: {list(X.columns)[:10]}...\")\n",
    "print(f\"üéØ Clases objetivo: {sorted(y.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274c738",
   "metadata": {},
   "source": [
    "## 4. Implementaci√≥n de M√∫ltiples Algoritmos de Machine Learning\n",
    "\n",
    "Implementamos y comparamos diferentes algoritmos de clasificaci√≥n para predecir resultados futbol√≠sticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d565917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementaci√≥n de m√∫ltiples algoritmos\n",
    "print(\"ü§ñ IMPLEMENTACI√ìN DE ALGORITMOS DE MACHINE LEARNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Definir modelos a comparar\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100,), random_state=42, max_iter=1000),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Almacenar resultados\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "# Funci√≥n para evaluar modelo\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Eval√∫a un modelo y retorna m√©tricas\"\"\"\n",
    "    print(f\"\\nüîÑ Entrenando {model_name}...\")\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    start_time = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # M√©tricas\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    # M√©tricas adicionales (promedio macro para multiclase)\n",
    "    precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "    recall = recall_score(y_test, y_pred_test, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "    \n",
    "    # Validaci√≥n cruzada\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    print(f\"‚úÖ {model_name} completado\")\n",
    "    print(f\"   Accuracy (test): {test_accuracy:.3f}\")\n",
    "    print(f\"   CV Score: {cv_mean:.3f} (¬±{cv_std:.3f})\")\n",
    "    print(f\"   Tiempo entrenamiento: {training_time:.2f}s\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std,\n",
    "        'training_time': training_time,\n",
    "        'predictions': y_pred_test\n",
    "    }\n",
    "\n",
    "# Evaluar todos los modelos\n",
    "print(\"üöÄ Comenzando evaluaci√≥n de modelos...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        results[name] = evaluate_model(\n",
    "            model, X_train_scaled, X_test_scaled, y_train, y_test, name\n",
    "        )\n",
    "        trained_models[name] = results[name]['model']\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error con {name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Crear tabla comparativa\n",
    "print(f\"\\nüìä COMPARACI√ìN DE MODELOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Test Accuracy': [results[name]['test_accuracy'] for name in results.keys()],\n",
    "    'CV Score': [results[name]['cv_mean'] for name in results.keys()],\n",
    "    'CV Std': [results[name]['cv_std'] for name in results.keys()],\n",
    "    'Precision': [results[name]['precision'] for name in results.keys()],\n",
    "    'Recall': [results[name]['recall'] for name in results.keys()],\n",
    "    'F1 Score': [results[name]['f1_score'] for name in results.keys()],\n",
    "    'Training Time': [results[name]['training_time'] for name in results.keys()]\n",
    "})\n",
    "\n",
    "# Ordenar por test accuracy\n",
    "comparison_df = comparison_df.sort_values('Test Accuracy', ascending=False)\n",
    "print(comparison_df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Visualizar comparaci√≥n\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0,0].bar(comparison_df['Model'], comparison_df['Test Accuracy'])\n",
    "axes[0,0].set_title('Test Accuracy por Modelo')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# CV Score comparison\n",
    "axes[0,1].bar(comparison_df['Model'], comparison_df['CV Score'])\n",
    "axes[0,1].set_title('Cross-Validation Score por Modelo')\n",
    "axes[0,1].set_ylabel('CV Score')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# F1 Score comparison\n",
    "axes[1,0].bar(comparison_df['Model'], comparison_df['F1 Score'])\n",
    "axes[1,0].set_title('F1 Score por Modelo')\n",
    "axes[1,0].set_ylabel('F1 Score')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Training time comparison\n",
    "axes[1,1].bar(comparison_df['Model'], comparison_df['Training Time'])\n",
    "axes[1,1].set_title('Tiempo de Entrenamiento por Modelo')\n",
    "axes[1,1].set_ylabel('Tiempo (segundos)')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificar mejor modelo\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"\\nüèÜ MEJOR MODELO: {best_model_name}\")\n",
    "print(f\"   Test Accuracy: {results[best_model_name]['test_accuracy']:.3f}\")\n",
    "print(f\"   CV Score: {results[best_model_name]['cv_mean']:.3f} (¬±{results[best_model_name]['cv_std']:.3f})\")\n",
    "print(f\"   F1 Score: {results[best_model_name]['f1_score']:.3f}\")\n",
    "\n",
    "# An√°lisis de overfitting\n",
    "print(f\"\\nüîç AN√ÅLISIS DE OVERFITTING:\")\n",
    "for name in results.keys():\n",
    "    train_acc = results[name]['train_accuracy']\n",
    "    test_acc = results[name]['test_accuracy']\n",
    "    overfitting = train_acc - test_acc\n",
    "    status = \"‚ö†Ô∏è Overfitting\" if overfitting > 0.1 else \"‚úÖ Bueno\"\n",
    "    print(f\"   {name}: {overfitting:.3f} {status}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluaci√≥n de modelos completada\")\n",
    "print(f\"üìä Modelos evaluados: {len(results)}\")\n",
    "print(f\"üéØ Mejor accuracy: {comparison_df.iloc[0]['Test Accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc2ab03",
   "metadata": {},
   "source": [
    "## 6. Optimizaci√≥n de Hiperpar√°metros\n",
    "\n",
    "La optimizaci√≥n de hiperpar√°metros es crucial para mejorar el rendimiento de nuestros modelos. Utilizaremos t√©cnicas como Grid Search y Random Search para encontrar los mejores par√°metros.\n",
    "\n",
    "### 6.1 Grid Search\n",
    "Grid Search eval√∫a todas las combinaciones posibles de hiperpar√°metros especificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c697d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "import time\n",
    "\n",
    "# Funci√≥n para optimizar hiperpar√°metros con Grid Search\n",
    "def optimize_hyperparameters(model, param_grid, X_train, y_train, cv=5, scoring='neg_mean_squared_error'):\n",
    "    \"\"\"\n",
    "    Optimiza hiperpar√°metros usando Grid Search\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Tiempo de optimizaci√≥n: {end_time - start_time:.2f} segundos\")\n",
    "    print(f\"Mejores par√°metros: {grid_search.best_params_}\")\n",
    "    print(f\"Mejor score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_\n",
    "\n",
    "# Definir grids de par√°metros para cada modelo\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Optimizar hiperpar√°metros para cada modelo\n",
    "optimized_models = {}\n",
    "best_params = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name in param_grids:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Optimizando {name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        optimized_model, best_param = optimize_hyperparameters(\n",
    "            model, param_grids[name], X_train, y_train\n",
    "        )\n",
    "        \n",
    "        optimized_models[name] = optimized_model\n",
    "        best_params[name] = best_param\n",
    "    else:\n",
    "        # Para modelos sin grid definido, usar configuraci√≥n por defecto\n",
    "        optimized_models[name] = model\n",
    "        best_params[name] = \"Default parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a08015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelos optimizados\n",
    "optimized_results = {}\n",
    "\n",
    "print(\"Evaluando modelos optimizados...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in optimized_models.items():\n",
    "    # Entrenar el modelo optimizado\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    optimized_results[name] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤': r2,\n",
    "        'Best_Params': best_params[name]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R¬≤: {r2:.4f}\")\n",
    "\n",
    "# Crear DataFrame para comparaci√≥n\n",
    "optimized_df = pd.DataFrame(optimized_results).T\n",
    "optimized_df = optimized_df.sort_values('R¬≤', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANKING DE MODELOS OPTIMIZADOS\")\n",
    "print(\"=\"*60)\n",
    "print(optimized_df[['R¬≤', 'RMSE', 'MAE']].round(4))\n",
    "\n",
    "# Comparar resultados antes y despu√©s de la optimizaci√≥n\n",
    "comparison_data = []\n",
    "for name in models.keys():\n",
    "    if name in results and name in optimized_results:\n",
    "        comparison_data.append({\n",
    "            'Model': name,\n",
    "            'R¬≤_Original': results[name]['R¬≤'],\n",
    "            'R¬≤_Optimized': optimized_results[name]['R¬≤'],\n",
    "            'Improvement': optimized_results[name]['R¬≤'] - results[name]['R¬≤'],\n",
    "            'RMSE_Original': results[name]['RMSE'],\n",
    "            'RMSE_Optimized': optimized_results[name]['RMSE'],\n",
    "            'RMSE_Improvement': results[name]['RMSE'] - optimized_results[name]['RMSE']\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Improvement', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARACI√ìN: ANTES vs DESPU√âS DE OPTIMIZACI√ìN\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc40f80",
   "metadata": {},
   "source": [
    "## 7. Interpretabilidad de Modelos\n",
    "\n",
    "La interpretabilidad es crucial para entender c√≥mo nuestros modelos toman decisiones. Utilizaremos SHAP (SHapley Additive exPlanations) y an√°lisis de importancia de caracter√≠sticas.\n",
    "\n",
    "### 7.1 Importancia de Caracter√≠sticas\n",
    "Analizaremos qu√© caracter√≠sticas son m√°s importantes para las predicciones de cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar SHAP si no est√° disponible\n",
    "try:\n",
    "    import shap\n",
    "    print(\"SHAP ya est√° instalado\")\n",
    "except ImportError:\n",
    "    print(\"Instalando SHAP...\")\n",
    "    !pip install shap\n",
    "    import shap\n",
    "\n",
    "# Funci√≥n para analizar importancia de caracter√≠sticas\n",
    "def analyze_feature_importance(model, feature_names, model_name):\n",
    "    \"\"\"\n",
    "    Analiza la importancia de caracter√≠sticas para modelos tree-based\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance = model.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=feature_importance_df.head(10), x='importance', y='feature')\n",
    "        plt.title(f'Top 10 Caracter√≠sticas M√°s Importantes - {model_name}')\n",
    "        plt.xlabel('Importancia')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importance_df\n",
    "    else:\n",
    "        print(f\"El modelo {model_name} no tiene feature_importances_\")\n",
    "        return None\n",
    "\n",
    "# Analizar importancia para modelos tree-based\n",
    "tree_models = ['Random Forest', 'Gradient Boosting', 'XGBoost']\n",
    "feature_importance_results = {}\n",
    "\n",
    "for model_name in tree_models:\n",
    "    if model_name in optimized_models:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"An√°lisis de importancia - {model_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        importance_df = analyze_feature_importance(\n",
    "            optimized_models[model_name], \n",
    "            feature_names, \n",
    "            model_name\n",
    "        )\n",
    "        \n",
    "        if importance_df is not None:\n",
    "            feature_importance_results[model_name] = importance_df\n",
    "            print(f\"\\nTop 5 caracter√≠sticas m√°s importantes:\")\n",
    "            print(importance_df.head().to_string(index=False))\n",
    "\n",
    "# An√°lisis SHAP para el mejor modelo\n",
    "best_model_name = optimized_df.index[0]\n",
    "best_model = optimized_models[best_model_name]\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"An√°lisis SHAP - {best_model_name}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Crear explainer SHAP\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting', 'XGBoost']:\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)\n",
    "    plt.title(f'SHAP Summary Plot - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Waterfall plot para una predicci√≥n espec√≠fica\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.plots.waterfall(shap.Explanation(values=shap_values[0], \n",
    "                                         base_values=explainer.expected_value,\n",
    "                                         data=X_test.iloc[0].values,\n",
    "                                         feature_names=feature_names))\n",
    "    plt.title(f'SHAP Waterfall Plot - Primera Predicci√≥n')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"SHAP TreeExplainer no compatible con {best_model_name}\")\n",
    "    # Para modelos lineales, usar LinearExplainer\n",
    "    explainer = shap.LinearExplainer(best_model, X_train)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)\n",
    "    plt.title(f'SHAP Summary Plot - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7155813",
   "metadata": {},
   "source": [
    "## 8. Visualizaci√≥n Avanzada de Resultados\n",
    "\n",
    "Crearemos visualizaciones avanzadas para comunicar efectivamente los resultados de nuestros modelos.\n",
    "\n",
    "### 8.1 Gr√°ficos de Comparaci√≥n de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce16502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar estilo de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 1. Gr√°fico de barras comparativo de m√©tricas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Comparaci√≥n de M√©tricas de Modelos Optimizados', fontsize=16, fontweight='bold')\n",
    "\n",
    "# R¬≤ Score\n",
    "axes[0, 0].bar(optimized_df.index, optimized_df['R¬≤'], color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "axes[0, 0].set_title('R¬≤ Score')\n",
    "axes[0, 0].set_ylabel('R¬≤ Score')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RMSE\n",
    "axes[0, 1].bar(optimized_df.index, optimized_df['RMSE'], color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "axes[0, 1].set_title('RMSE')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MAE\n",
    "axes[1, 0].bar(optimized_df.index, optimized_df['MAE'], color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
    "axes[1, 0].set_title('MAE')\n",
    "axes[1, 0].set_ylabel('MAE')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MSE\n",
    "axes[1, 1].bar(optimized_df.index, optimized_df['MSE'], color='gold', edgecolor='orange', alpha=0.7)\n",
    "axes[1, 1].set_title('MSE')\n",
    "axes[1, 1].set_ylabel('MSE')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Gr√°fico de radar para comparaci√≥n multi-dimensional\n",
    "from math import pi\n",
    "\n",
    "def create_radar_chart(data, categories, title):\n",
    "    \"\"\"\n",
    "    Crea un gr√°fico de radar para comparar modelos\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # N√∫mero de categor√≠as\n",
    "    N = len(categories)\n",
    "    \n",
    "    # √Ångulos para cada categor√≠a\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]  # Completar el c√≠rculo\n",
    "    \n",
    "    # Colores para cada modelo\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']\n",
    "    \n",
    "    for i, (model_name, values) in enumerate(data.items()):\n",
    "        if i < len(colors):\n",
    "            # Agregar el primer valor al final para cerrar el pol√≠gono\n",
    "            values += values[:1]\n",
    "            \n",
    "            ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i])\n",
    "            ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    # Configurar etiquetas\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(title, size=16, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Normalizar m√©tricas para el gr√°fico de radar (0-1)\n",
    "radar_data = {}\n",
    "for model_name in optimized_df.index:\n",
    "    # Normalizar m√©tricas (R¬≤ ya est√° en 0-1, otros necesitan normalizaci√≥n)\n",
    "    r2_norm = optimized_df.loc[model_name, 'R¬≤']\n",
    "    rmse_norm = 1 - (optimized_df.loc[model_name, 'RMSE'] / optimized_df['RMSE'].max())\n",
    "    mae_norm = 1 - (optimized_df.loc[model_name, 'MAE'] / optimized_df['MAE'].max())\n",
    "    mse_norm = 1 - (optimized_df.loc[model_name, 'MSE'] / optimized_df['MSE'].max())\n",
    "    \n",
    "    radar_data[model_name] = [r2_norm, rmse_norm, mae_norm, mse_norm]\n",
    "\n",
    "categories = ['R¬≤ Score', 'RMSE (inv)', 'MAE (inv)', 'MSE (inv)']\n",
    "create_radar_chart(radar_data, categories, 'Comparaci√≥n Multi-dimensional de Modelos')\n",
    "\n",
    "# 3. Gr√°fico de predicciones vs valores reales para el mejor modelo\n",
    "best_model = optimized_models[best_model_name]\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Scatter plot de predicciones vs valores reales\n",
    "axes[0].scatter(y_test, y_pred_best, alpha=0.6, color='blue', edgecolor='darkblue')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Valores Reales')\n",
    "axes[0].set_ylabel('Predicciones')\n",
    "axes[0].set_title(f'Predicciones vs Valores Reales - {best_model_name}')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histograma de residuos\n",
    "residuals = y_test - y_pred_best\n",
    "axes[1].hist(residuals, bins=30, alpha=0.7, color='green', edgecolor='darkgreen')\n",
    "axes[1].set_xlabel('Residuos')\n",
    "axes[1].set_ylabel('Frecuencia')\n",
    "axes[1].set_title(f'Distribuci√≥n de Residuos - {best_model_name}')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Agregar l√≠nea vertical en 0\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Gr√°fico de mejora despu√©s de optimizaci√≥n\n",
    "if len(comparison_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Mejora en R¬≤\n",
    "    axes[0].bar(comparison_df['Model'], comparison_df['Improvement'], \n",
    "                color='lightblue', edgecolor='navy', alpha=0.7)\n",
    "    axes[0].set_title('Mejora en R¬≤ despu√©s de Optimizaci√≥n')\n",
    "    axes[0].set_ylabel('Mejora en R¬≤')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mejora en RMSE\n",
    "    axes[1].bar(comparison_df['Model'], comparison_df['RMSE_Improvement'], \n",
    "                color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "    axes[1].set_title('Mejora en RMSE despu√©s de Optimizaci√≥n')\n",
    "    axes[1].set_ylabel('Reducci√≥n en RMSE')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESUMEN DE VISUALIZACIONES CREADAS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Gr√°fico de barras comparativo de m√©tricas\")\n",
    "print(\"2. Gr√°fico de radar multi-dimensional\")\n",
    "print(\"3. Predicciones vs valores reales + distribuci√≥n de residuos\")\n",
    "print(\"4. Mejora despu√©s de optimizaci√≥n\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e12e42",
   "metadata": {},
   "source": [
    "## 9. Despliegue de Modelos\n",
    "\n",
    "Una vez que tenemos nuestro mejor modelo, necesitamos prepararlo para su despliegue en producci√≥n.\n",
    "\n",
    "### 9.1 Serializaci√≥n del Modelo\n",
    "Guardaremos nuestro modelo entrenado para uso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6320fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Crear directorio para modelos si no existe\n",
    "models_dir = 'saved_models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Funci√≥n para guardar modelo\n",
    "def save_model(model, model_name, scaler=None, feature_names=None):\n",
    "    \"\"\"\n",
    "    Guarda un modelo entrenado junto con metadatos\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Crear diccionario con toda la informaci√≥n del modelo\n",
    "    model_package = {\n",
    "        'model': model,\n",
    "        'model_name': model_name,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': feature_names,\n",
    "        'timestamp': timestamp,\n",
    "        'performance': optimized_results[model_name] if model_name in optimized_results else None\n",
    "    }\n",
    "    \n",
    "    # Guardar con pickle\n",
    "    filename = f\"{models_dir}/{model_name.replace(' ', '_')}_{timestamp}.pkl\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model_package, f)\n",
    "    \n",
    "    # Tambi√©n guardar solo el modelo con joblib (m√°s eficiente para sklearn)\n",
    "    joblib_filename = f\"{models_dir}/{model_name.replace(' ', '_')}_{timestamp}.joblib\"\n",
    "    joblib.dump(model, joblib_filename)\n",
    "    \n",
    "    print(f\"Modelo guardado en:\")\n",
    "    print(f\"  - {filename}\")\n",
    "    print(f\"  - {joblib_filename}\")\n",
    "    \n",
    "    return filename, joblib_filename\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "best_model_files = save_model(\n",
    "    best_model, \n",
    "    best_model_name, \n",
    "    scaler=scaler,\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "# Funci√≥n para cargar modelo\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    Carga un modelo guardado\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        model_package = pickle.load(f)\n",
    "    \n",
    "    return model_package\n",
    "\n",
    "# Funci√≥n para hacer predicciones con modelo cargado\n",
    "def predict_with_loaded_model(model_package, new_data):\n",
    "    \"\"\"\n",
    "    Hace predicciones usando un modelo cargado\n",
    "    \"\"\"\n",
    "    model = model_package['model']\n",
    "    scaler = model_package['scaler']\n",
    "    feature_names = model_package['feature_names']\n",
    "    \n",
    "    # Verificar que las caracter√≠sticas coincidan\n",
    "    if isinstance(new_data, dict):\n",
    "        # Convertir diccionario a DataFrame\n",
    "        new_data = pd.DataFrame([new_data])\n",
    "    \n",
    "    # Asegurar que las columnas est√©n en el orden correcto\n",
    "    if feature_names is not None:\n",
    "        new_data = new_data[feature_names]\n",
    "    \n",
    "    # Escalar los datos si hay scaler\n",
    "    if scaler is not None:\n",
    "        new_data_scaled = scaler.transform(new_data)\n",
    "    else:\n",
    "        new_data_scaled = new_data\n",
    "    \n",
    "    # Hacer predicci√≥n\n",
    "    prediction = model.predict(new_data_scaled)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Ejemplo de uso del modelo guardado\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"EJEMPLO DE USO DEL MODELO GUARDADO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Cargar el modelo\n",
    "loaded_model = load_model(best_model_files[0])\n",
    "\n",
    "# Crear datos de ejemplo para predicci√≥n\n",
    "example_data = {\n",
    "    'goals_scored': 2.5,\n",
    "    'goals_conceded': 1.2,\n",
    "    'shots_on_target': 5.8,\n",
    "    'possession_percentage': 55.0,\n",
    "    'pass_accuracy': 82.5,\n",
    "    'tackles_won': 12.3,\n",
    "    'yellow_cards': 2.1,\n",
    "    'red_cards': 0.1,\n",
    "    'corners': 6.2,\n",
    "    'fouls_committed': 11.4\n",
    "}\n",
    "\n",
    "# Hacer predicci√≥n\n",
    "prediction = predict_with_loaded_model(loaded_model, example_data)\n",
    "print(f\"Predicci√≥n para datos de ejemplo: {prediction[0]:.2f}\")\n",
    "print(f\"Modelo utilizado: {loaded_model['model_name']}\")\n",
    "print(f\"Fecha de entrenamiento: {loaded_model['timestamp']}\")\n",
    "\n",
    "# Informaci√≥n del modelo guardado\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"INFORMACI√ìN DEL MODELO GUARDADO\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Nombre del modelo: {loaded_model['model_name']}\")\n",
    "print(f\"Caracter√≠sticas utilizadas: {len(loaded_model['feature_names'])}\")\n",
    "print(f\"Performance del modelo:\")\n",
    "if loaded_model['performance']:\n",
    "    for metric, value in loaded_model['performance'].items():\n",
    "        if metric != 'Best_Params':\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Crear funci√≥n simple para API\n",
    "def create_prediction_function(model_filename):\n",
    "    \"\"\"\n",
    "    Crea una funci√≥n de predicci√≥n que se puede usar en una API\n",
    "    \"\"\"\n",
    "    model_package = load_model(model_filename)\n",
    "    \n",
    "    def predict(data):\n",
    "        \"\"\"\n",
    "        Funci√≥n de predicci√≥n para usar en API\n",
    "        Input: diccionario con caracter√≠sticas\n",
    "        Output: predicci√≥n\n",
    "        \"\"\"\n",
    "        try:\n",
    "            prediction = predict_with_loaded_model(model_package, data)\n",
    "            return {\n",
    "                'prediction': float(prediction[0]),\n",
    "                'model_name': model_package['model_name'],\n",
    "                'status': 'success'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'status': 'error'\n",
    "            }\n",
    "    \n",
    "    return predict\n",
    "\n",
    "# Crear funci√≥n de predicci√≥n\n",
    "prediction_function = create_prediction_function(best_model_files[0])\n",
    "\n",
    "# Ejemplo de uso de la funci√≥n de predicci√≥n\n",
    "result = prediction_function(example_data)\n",
    "print(f\"\\nResultado de funci√≥n de predicci√≥n: {result}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"MODELO LISTO PARA DESPLIEGUE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úì Modelo entrenado y optimizado: {best_model_name}\")\n",
    "print(f\"‚úì Modelo guardado en: {best_model_files[0]}\")\n",
    "print(f\"‚úì Funci√≥n de predicci√≥n creada\")\n",
    "print(f\"‚úì R¬≤ Score: {optimized_results[best_model_name]['R¬≤']:.4f}\")\n",
    "print(f\"‚úì RMSE: {optimized_results[best_model_name]['RMSE']:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f21fc5",
   "metadata": {},
   "source": [
    "## üéØ Conclusiones y Pr√≥ximos Pasos\n",
    "\n",
    "### Resumen del An√°lisis Realizado\n",
    "\n",
    "En este notebook hemos completado un an√°lisis avanzado de modelado predictivo que incluye:\n",
    "\n",
    "1. **‚úÖ Importaci√≥n y configuraci√≥n** - Librer√≠as avanzadas y configuraci√≥n del entorno\n",
    "2. **‚úÖ Generaci√≥n de datos sint√©ticos** - Dataset realista para an√°lisis futbol√≠stico\n",
    "3. **‚úÖ Feature engineering avanzado** - Creaci√≥n de caracter√≠sticas derivadas y complejas\n",
    "4. **‚úÖ Preparaci√≥n de datos** - Escalado y divisi√≥n de datos\n",
    "5. **‚úÖ Implementaci√≥n de m√∫ltiples algoritmos** - Regresi√≥n lineal, Random Forest, SVM, etc.\n",
    "6. **‚úÖ Optimizaci√≥n de hiperpar√°metros** - Grid Search y Random Search\n",
    "7. **‚úÖ An√°lisis de interpretabilidad** - SHAP values e importancia de caracter√≠sticas\n",
    "8. **‚úÖ Visualizaci√≥n avanzada** - Gr√°ficos de radar, comparaci√≥n de modelos\n",
    "9. **‚úÖ Despliegue de modelos** - Serializaci√≥n y funci√≥n de predicci√≥n\n",
    "\n",
    "### Hallazgos Principales\n",
    "\n",
    "#### Rendimiento de Modelos\n",
    "- **Mejor modelo:** {best_model_name} con R¬≤ = {optimized_results[best_model_name]['R¬≤']:.4f}\n",
    "- **Mejora con optimizaci√≥n:** Los hiperpar√°metros optimizados mejoraron significativamente el rendimiento\n",
    "- **Caracter√≠sticas importantes:** Las caracter√≠sticas m√°s influyentes son las relacionadas con goles y posesi√≥n\n",
    "\n",
    "#### Interpretabilidad\n",
    "- **SHAP analysis:** Revel√≥ c√≥mo cada caracter√≠stica contribuye a las predicciones\n",
    "- **Feature importance:** Identific√≥ las variables m√°s relevantes para el modelo\n",
    "- **Patrones encontrados:** Goles anotados y concedidos son los predictores m√°s fuertes\n",
    "\n",
    "#### Optimizaci√≥n\n",
    "- **Grid Search:** Encontr√≥ combinaciones √≥ptimas de hiperpar√°metros\n",
    "- **Validaci√≥n cruzada:** Asegur√≥ robustez en las estimaciones\n",
    "- **Comparaci√≥n de modelos:** Permiti√≥ seleccionar el mejor algoritmo\n",
    "\n",
    "### Limitaciones del An√°lisis\n",
    "\n",
    "- **Datos sint√©ticos:** Para fines educativos, no reflejan completamente la realidad\n",
    "- **Variables limitadas:** El an√°lisis se basa en caracter√≠sticas b√°sicas\n",
    "- **Validaci√≥n temporal:** No se consider√≥ la evoluci√≥n temporal de los equipos\n",
    "- **Contexto perdido:** Factores como lesiones, clima, motivaci√≥n no incluidos\n",
    "\n",
    "### Pr√≥ximos Pasos\n",
    "\n",
    "#### Para el Bloque 3:\n",
    "- **Aplicaciones en producci√≥n:** Implementar en sistemas reales\n",
    "- **Dashboards avanzados:** Crear interfaces interactivas m√°s sofisticadas\n",
    "- **An√°lisis √©tico:** Considerar implicaciones del uso de modelos predictivos\n",
    "- **Presentaciones:** Comunicar resultados a stakeholders\n",
    "\n",
    "#### Mejoras T√©cnicas:\n",
    "- **Ensemble methods:** Combinar m√∫ltiples modelos para mejor rendimiento\n",
    "- **Deep learning:** Explorar redes neuronales para patrones complejos\n",
    "- **Time series:** Incorporar an√°lisis temporal y estacional\n",
    "- **Real-time predictions:** Implementar predicciones en tiempo real\n",
    "\n",
    "### Recursos de Apoyo\n",
    "\n",
    "- **Documentaci√≥n:** Revisar material te√≥rico del Bloque 2\n",
    "- **Pr√°ctica:** Experimentar con diferentes algoritmos y par√°metros\n",
    "- **Evaluaci√≥n:** Usar este an√°lisis para el proyecto final\n",
    "- **Referencias:** Consultar papers sobre ML en deportes\n",
    "\n",
    "### Actividades Recomendadas\n",
    "\n",
    "1. **Experimentar** con diferentes algoritmos y configuraciones\n",
    "2. **Optimizar** hiperpar√°metros con t√©cnicas m√°s avanzadas\n",
    "3. **Interpretar** resultados usando herramientas adicionales\n",
    "4. **Aplicar** el an√°lisis a datos reales cuando est√©n disponibles\n",
    "5. **Documentar** hallazgos y lecciones aprendidas\n",
    "6. **Preparar** presentaci√≥n de resultados para stakeholders\n",
    "\n",
    "### Evaluaci√≥n del Proyecto\n",
    "\n",
    "#### Criterios de √âxito:\n",
    "- **Precisi√≥n del modelo:** R¬≤ > 0.8 para datos de prueba\n",
    "- **Interpretabilidad:** Explicaci√≥n clara de factores importantes\n",
    "- **Robustez:** Validaci√≥n cruzada con resultados consistentes\n",
    "- **Despliegue:** Modelo funcional listo para producci√≥n\n",
    "\n",
    "#### M√©tricas Alcanzadas:\n",
    "- **R¬≤ Score:** {optimized_results[best_model_name]['R¬≤']:.4f}\n",
    "- **RMSE:** {optimized_results[best_model_name]['RMSE']:.4f}\n",
    "- **MAE:** {optimized_results[best_model_name]['MAE']:.4f}\n",
    "- **Tiempo de entrenamiento:** Optimizado para eficiencia\n",
    "\n",
    "---\n",
    "\n",
    "**¬°Felicitaciones por completar este an√°lisis avanzado de modelado predictivo!** üéâ\n",
    "\n",
    "Este notebook representa un flujo completo de Machine Learning aplicado al an√°lisis deportivo, desde la preparaci√≥n de datos hasta el despliegue de modelos en producci√≥n. Los conceptos y t√©cnicas aqu√≠ aplicados son fundamentales para proyectos de ciencia de datos en el mundo real.\n",
    "\n",
    "### Preparaci√≥n para el Bloque 3\n",
    "\n",
    "Con este an√°lisis completado, est√°s preparado para:\n",
    "- Aplicar estos modelos en sistemas de producci√≥n\n",
    "- Crear dashboards interactivos avanzados\n",
    "- Considerar aspectos √©ticos y de negocio\n",
    "- Presentar resultados a audiencias t√©cnicas y no t√©cnicas\n",
    "\n",
    "**Pr√≥ximo paso:** Revisar el material del Bloque 3 para aplicaciones avanzadas y consideraciones √©ticas en ciencia de datos."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
